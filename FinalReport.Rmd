---
title: "STA 325 Case Study"
author: "Abby L., Ava E., Ella T., Grady P. Laura C."
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r, echo=FALSE, message=FALSE}
# Loading packages and data
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)
library(knitr)
library(patchwork)

# Loading the data
data <- read.csv("Data/data_transformed.csv")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction

Whether it’s stirring a morning cup of coffee or feeling your stomach lurch during a bumpy airplane ride, nearly everyone has experienced some form of turbulence. Beyond these everyday encounters, turbulence plays a crucial role in many complex natural and industrial processes, from air pollution and chemical reactions to heat transfer and weather systems. Despite its ubiquity, turbulence remains notoriously difficult to predict and has long been regarded as “the last great unsolved problem in classical physics.”

While we may not be physicists ourselves, we have undertaken the challenge of developing a predictive model to better understand this elusive phenomenon in collaboration with our Professor, Simon Mak. Given observations for fluid turbulence (quantified by Reynolds number Re), gravitational acceleration (quantified by Froud number Fr), and the particle's characteristics (quantified by Stokes number St), we have explored models designed to predict the mean, standard deviation, skewness, and kurtosis of the spatial distribution and clustering of particles in clouds in a state of idealized turbulence. Our goal is to create a model that balances predictive accuracy with interpretability; one that not only performs well statistically but also provides clear, meaningful insights into how different conditions influence turbulent behavior.

# Methodology

Observing the distributions of the predictor variables, the square root of St was used as well as the log of Re to improve the symmetry of the distribution to address the constant variance assumption. Additionally, the logit function was applied to Fr so that all values are bound between 0 and 1.

One concern with the data is that Re and Fr each only have 3 unique values. Hence, using these variables to model the distribution of the response variable requires a large amount of uncertainty and interpolation at values of these variables between the 3 distinct values in the data. To continue refining the models, additional data with different values for these models will reduce uncertainty and improve model fit.

Additionally, we analyzed the distributions of each response variable before building the model. To correct for asymmetries and heteroskedasticity amongst the residuals, we applied different transformations to some of the response variables: log transforming the Mean, log transforming the Standard Deviation, and square rooting Skew. These transformations were determined by analyzing the distributions of these response variables as well as results from the Box-Cox transformation. Since we fit initial models with linear regression as well as using linear regression for coefficient interpretation for certain models, transforming Mean and Skew improved the distribution of the noise to closer resemble a normal distribution with the residuals randomly scattered around 0, satisfying the assumption that the error is normally distributed in linear regression.

# Results

## Mean

To predict the mean particle cluster volume, several models were assessed including simple linear regression, interaction terms, polynomial models, ridge regression, and spline. After performing 5-fold CV on each, the polynomial model on log(Mean) performed best (assessed using residual plots, $R^2$, AIC, and BIC). The log transformation was necessary because without it, the residuals showed clear patterns and heteroscedasticity.

The final model includes second-degree polynomial terms for all three predictors (√St, Re, and the logistic-transformed Fr) along with three two-way interactions: √St:Re, √St:logistic_Fr, and Re:logistic_Fr. This model achieved great fit, with an adjusted R² of 0.9978, meaning it explains over 99% of the variation in log(mean) cluster volume. The model's RMSE of 0.098 on the log scale translates to predictions that are typically within about 10% of the true mean cluster volume when back-transformed, although there are still some uncertainty regarding the predictions due to the presence of categorical predictors.

An ANOVA test revealed that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19) on explaining mean cluster volume. All three interaction terms were statistically significant (p \< 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p \< 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions. To further assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

```{r, echo = FALSE}
mean_model <- lm(log(mean) ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
    sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, data= data)

# new df that varies Re while fixing St and Fr at their means
newdata_mean <- with(data, data.frame(
  Re          = seq(min(Re, na.rm = TRUE), max(Re, na.rm = TRUE), length.out = 100),
  St          = rep(mean(St, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for Re values
pred_meanRe <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanRe <- cbind(newdata_mean, pred_meanRe)

plot_meanRe1 <- ggplot(plot_meanRe, aes(x= Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(mean))",
    x = "Re",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies St while fixing Re and Fr at their means
newdata_mean <- with(data, data.frame(
  St          = seq(min(St, na.rm = TRUE), max(St, na.rm = TRUE), length.out = 100),
  Re          = rep(mean(Re, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for St values
pred_meanSt <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanSt <- cbind(newdata_mean, pred_meanSt)

plot_meanSt1 <- ggplot(plot_meanSt, aes(x= St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "St",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies Fr while fixing Re and St at their means
newdata_mean <- with(data, data.frame(
  logistic_Fr = seq(min(data$logistic_Fr, na.rm = TRUE), max(data$logistic_Fr, na.rm = TRUE), length.out = 100),
  Re  = rep(mean(Re, na.rm = TRUE), 100),
  St = rep(mean(St, na.rm = TRUE), 100)
))

# 95% CI for Fr values
pred_meanFr <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanFr <- cbind(newdata_mean, pred_meanFr)

plot_meanFr1 <- ggplot(plot_meanFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic(Fr)",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

```

```{r, fig.width=9, fig.height=3}
(plot_meanRe1 | plot_meanSt1 | plot_meanFr1)
```

The plots depict the 95% CI for the predicted log(mean) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are very confident that there appears to be a strong negative relationship between Re and log(mean). For St, we are fairly confident that there is a nonlinear and positive relationship between St and log(mean). However, the wide confidence intervals at the edges depict some areas of uncertainty. For Fr, we have some confidence that the relationship between logistic(Fr) and log(mean) is U-shaped, such that at low and high values, log(mean)is higher and values in the middle are lower. There appears to be some uncertainty near the curvature, but the pattern appears to be overall evident of a non-monotonic effect of logistic_Fr on log(mean).

## Standard Deviation

```{r}
# EDA for standard deviation
p1 <- ggplot(data, aes(x = sd)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(sd ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The Box Cox transformation shows that the optimal lambda is around 0, so the log transformation on sd is the optimal transformation to address skewness of the response variable.

We were given a training and testing dataset, so I trained different linear (with and without ridge and lasso), polynomial and spline regression models comparing the adjusted R-squared the p-values, and used 5-fold CV to compare the top model to determine which model had the best prediction accuracy while working on new, unseen data.

After multiple models were compared from their adjusted R-squared and 5-fold CV, the linear model with all of the interaction terms was selected based on the balance between predictive accuracy (RMSE = 2.01, Adjusted R-squared = 0.74). The interaction terms were included to capture the potential effects of predictors coupled together. The linear model is interpretable and less risky to overfit, which is helpful in our goal to predict the standard deviations of the distributions given St, Re, and Fr.

```{r}
model_summary <- data.frame(
  Model = c(
    "lin","lin_interactions","lin_interactions_subset","poly","spline","lasso",
    "ridge"
  ),
  RMSE = c(2.077942, 2.013816, 2.166138, 2.072878,2.115911,2.068582, 2.080013)
)

kable(model_summary)

```

When running 5-fold CV, the RMSE for the selected model is 2.0138, which is lower compared to other linear, ridge, lasso, spline and polynomial models.

```{r, echo = FALSE}
#COULD DELETE THIS MODEL
lm.fit <- lm(log(sd) ~ (log_St+log_Re+logistic_Fr)^2, data)
# plot(lm.fit)
# summary(lm.fit)
```

```{r, echo= FALSE}
sd_model <- lm(log(sd) ~ log_St + log_Re + logistic_Fr +
              log_St:log_Re + log_St:logistic_Fr + log_Re:logistic_Fr,
              data = data)
```

```{r, echo = FALSE}
newdata_sd <- data.frame(
  log_St = seq(min(data$log_St), max(data$log_St), length.out = 100),
  log_Re = mean(data$log_Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdSt <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdSt <- cbind(newdata_sd, pred_sdSt)

plot_sdSt1 <- ggplot(plot_sdSt, aes(x= log_St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (log(SD))",
    x = "log(St)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  log_Re = seq(min(data$log_Re), max(data$log_Re), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdRe <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdRe <- cbind(newdata_sd, pred_sdRe)

plot_sdRe1 <- ggplot(plot_sdRe, aes(x= log_Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "log(Re)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  log_Re = mean(data$log_Re, na.rm = TRUE)
)

pred_sdFr <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdFr <- cbind(newdata_sd, pred_sdFr)

plot_sdFr1 <- ggplot(plot_sdFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic(Fr)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()
```

```{r, fig.width=9, fig.height=3}
(plot_sdSt1 | plot_sdRe1 | plot_sdFr1)
```

The plots depict the 95% CI for the predicted log(sd) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are fairly confident that there appears to be a positive relationship between log(St) and log(SD), with greater variability near the extremes. For Re, we are very confident that there is a strong negative relationship between log(Re) and log(SD), such that at larger Re values, there is less variability in log(Re). For Fr, we have some confidence that the relationship between logistic(Fr) and log(SD) is negative; however, the confidence band is much wider compared to the other predictors, which means that there does appear to be more uncertainty.

```{r}
summary(lm.fit)
```

From our model, we can see that log_Re, logistic_Fr, and the interaction between log_Re and logistic_Fr has significant effects on the standard deviation of the particle cluster volume distribution. For example, holding other factors constant, for a 1% increase in the Reynold's Number, there would be a 4.915% in decrease standard deviation. Also, holding other factors constant, for a 1% increase in the Stokes number, there would be a 20.704% decrease in standard deviation. In context, this means that more turbulent regimes and regimes with more dense particles lead to narrower particle cluster distributions. However, the effect with Reynold's Number is reduces in regimes with higher gravitational acceleration, or Fr, as seen with the interaction effect between log(Re) and logistic(Fr).

## Skew

Looking at the distribution of a Skew and a Box-Cox transformation, the square root transformation is selected to address the skewness of the response variable.

```{r, fig.height53, fig.width=3}
# EDA for skew
p1 <- ggplot(data, aes(x = skewness)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Skew", x = "Skew", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(skewness ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

After evaluating linear regression, polynomial models, ridge regression, and splines, the model which best fit the √(Skew) response is a ridge model fit using 5-fold cross validation. To evaluate the significance of coefficients, the same predictors were fit with OLS to develop better intuition regarding the magnitude and significance of coefficients.

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_sig <-
  data.frame(
    Predictor = c("logistic_Fr", "Re:logistic_Fr"),
    Estimate = c(-21.35, 0.055)
  )
kable(skew_sig)
```

From the OLS model, the above coefficients were determined to be the most significant:

-   Transforming logistic_Fr back to Fr yields a coefficient extremely close to 0. This indicates that changes in the gravitational acceleration of particles (height of the cloud) does not greatly change the asymmetry of the turbulence distribution.

-   Although Fr by itself has a basically negligible impact on the distribution of √(Skew), the interaction between Fr and Re is statistically significant. Lower-hanging clouds have a higher Fr value so a positive interaction between Re and logistic_Fr indicates that either as a cloud's Fr increases (the cloud get lower) or Re increases (the flow is more turbulent), √(Skew) is increasing. Given that logistic_Fr has a negative coefficient yet the interaction between Re and logistic_Fr is positive, exploring the relationship between the height of the cloud and the turbulence of the flow warrants further investigation.

In a separate model with the same linear combinations of predictors, logistic_Fr and Re were treated categorically. Using categorical variables greatly improved the model performance (quantified by $R^2$, AIC, BIC, and Cp). Despite a categorical model being an unrealistic fit for data with continuous values, the model still provided insights into the interactions between these variables:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_cat <-
  data.frame(
    Predictor = c("sqrt(St):as.factor(Re)224", "sqrt(St):as.factor(Re)398  "),
    Estimate = c(-1.4173, -2.5683)
  )
kable(skew_cat)
```

-   When treated as categorical variables, the relationships which are most significant both involve √(St) and the levels of Re.

-   Interpretation: When the Reynolds' Number increases from the baseline level to 224, the expected square root of Skew decreases holding √(St) constant. An even larger decrease is observed when the Reynolds' Number increases to 398 and √(St) is held constant. As randomness increases and the size of the particles remains the same, we expect to see a decrease the asymmetry of the distribution of the turbulence so the distribution of the clustering of the particles appears more symmetrical with an increase in randomness.

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model.

```{r, fig.width=9, fig.height=3}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```

## Kurtosis

The best model for Kurtosis is a polynomial model for the untransformed values of Kurtosis where √(St) is not used in a polynomial term while both Re and logistic_Fr are raised to the second power. There are also interactions terms included between all three variables and their polynomial terms.

This polynomial model performed well when evaluated using 5-fold CV with a fairly high $R^2$ value and fairly low RMSE, providing us with confidence that this model is not overfit to the training data. Many of the coefficients are extremely significant in this model. For the final polynomial model, logistic_Fr is again statistically significant for both degrees which was also observed for Skewness above. However, we do notice some additional statistically significant interactions which are worth mentioning:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
kurt_cat <-
  data.frame(
    Predictor = c("sqrt(St)", "poly(Re, 2)1", "poly(Re, 2)1:poly(logistic_Fr, 2)1"),
    Estimate = c(-16383, 204158, 888507)
  )
kable(kurt_cat)

```

-   The negative sign on sqrt(St) coefficient indicates that as the size of the particles in the environment increase, the kurtosis of the distribution decreases. When we simulate from larger particles, the turbulence distribution becomes less sharp as it's not as concentrated around a single value. One conclusion from this may be that increasing the size the particles increases the range of the distribution of particle turbulence resulting in less heavy tails for the distribution.

-   The first degree for Re is largely positive indicating an increase in the turbulence of the flow of the particles increases how heavy the tails of the clustering distribution are.

-   Lastly, there is a significant interaction between the first degrees for both Re and Fr which can be interpreted in two ways. Firstly, increasing the turbulence of the flow while holding the gravitational acceleration constant, is expected to increase the Kurtosis of the model as the clustering distribution becomes more peaked. Alternatively, increasing the gravitational acceleration while holding turbulence constant, is expected to also increase the Kurtosis of the model. Intuitively, one would imagine that first interpretation is more intuitive given that increasing flow turbulence seems to have a more significant impact given the bullet above, but further discussion with the collaborator and model investigation would be worthwhile to determine the true effects.

We see a similar statistical significance between √(St) and the categorical values of Re for the polynomial terms.

# Conclusion

We would like to thank our collaborators in Professor Simon Mak and the Duke Civil & Environmental Engineering department. One can access the code repository for this project via this link: [Github Repository](https://github.com/avaexelbirt/STA-325-Case-Study){.uri}.
