---
title: "STA 325 Case Study"
author: "Abby L., Ava E., Ella T., Grady P. Laura C."
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r, echo=FALSE, message=FALSE}
# Loading packages and data
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)
library(knitr)
library(patchwork)

# Loading the data
data <- read.csv("Data/data_transformed.csv")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```


# Introduction

Whether it’s stirring a morning cup of coffee or feeling your stomach lurch during a bumpy airplane ride, nearly everyone has experienced some form of turbulence. Beyond these everyday encounters, turbulence plays a crucial role in many complex natural and industrial processes, from air pollution and chemical reactions to heat transfer and weather systems. Despite its ubiquity, turbulence remains notoriously difficult to predict and has long been regarded as “the last great unsolved problem in classical physics.”

While we may not be physicists ourselves, as members of Stat 325, we have undertaken the challenge of developing a predictive model to better understand this elusive phenomenon in collaboration with our Professor, Simon Mak. Given observations for fluid turbulence (quantified by Reynolds number Re), gravitational acceleration (quantified by Froud number Fr), and the particle's characteristics (quantified by Stokes number St), we have explored models designed to predict the mean, standard deviation, skewness, and kurtosis of the spatial distribution and clustering of particles in clouds in a state of idealized turbulence. Our goal is to create a model that balances predictive accuracy with interpretability; one that not only performs well statistically but also provides clear, meaningful insights into how different conditions influence turbulent behavior.

# Methodology
Observing the distributions of the response variables, the square root of St was used as well as the log of Re to improve the symmetry of the distribution to address the constant variance assumption. Additionally, the logit function was applied to Fr so that all values are bound between 0 and 1. 

One concern with the data is that Re and Fr each only have 3 unique values. Hence, using these variables to model the distribution of the response variable requires a large amount of uncertainty and interpolation at values of these variables between the 3 distinct values in the data. To continue refining the models, additional data with different values for these models will reduce uncertainty and improve model fit.

## Mean 
To predict the mean particle cluster volume, we assessed the performance of a few different models including simple linear regression, interaction terms, polynomial models, ridge regression, and spline. We then assessed the fit of each by performing 5-fold CV. The polynomial model performed best. Then, we performed a log transform on the response variable. Our final model is the polynomial regression model on the log-transformed mean. The log transformation was necessary because without it, the residuals showed clear patterns and heteroscedasticity. After transformation, the residuals appear to be randomly scattered around zero with constant variance. The log transformation also addressed the right-skewed distribution of mean cluster volumes in the raw data.
   
$$
\log(\text{mean}) = \beta_0 
+ \beta_1 \, \text{poly}_1(\sqrt{St}) 
+ \beta_2 \, \text{poly}_2(\sqrt{St}) 
+ \beta_3 \, \text{poly}_1(Re) 
+ \beta_4 \, \text{poly}_2(Re) 
+ \beta_5 \, \text{poly}_1(\text{logisticFr}) 
+ \beta_6 \, \text{poly}_2(\text{logisticFr}) 
+ \beta_7 \, (\sqrt{St} \cdot Re) 
+ \beta_8 \, (\sqrt{St} \cdot \text{logisticFr}) 
+ \beta_9 \, (Re \cdot \text{logisticFr}) 
$$
The final model includes second-degree polynomial terms for all three predictors (√St, Re, and the logistic-transformed Fr) along with three two-way interactions: √St:Re, √St:logistic_Fr, and Re:logistic_Fr. This model achieved great fit, with an adjusted R² of 0.9978, meaning it explains over 99% of the variation in log(mean) cluster volume. The model's RMSE of 0.098 on the log scale translates to predictions that are typically within about 10% of the true mean cluster volume when back-transformed, although there are still some uncertainty regarding the predictions due to the presence of categorical predictors. 

We also ran ANOVA which reveals which predictors contribute most substantially to explaining mean cluster volume. The results show that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19). All three interaction terms were statistically significant (p \< 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p \< 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions. To furhter assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

## Standard Deviation

```{r}
# EDA for standard deviation
p1 <- ggplot(data, aes(x = sd)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(sd ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```
The Box Cox transformation shows that the optimal lambda is around 0, so the log transformation on sd is the optimal transformation to address skewness of the response variable. 

We were given a training and testing dataset, so I trained  different linear (with and without ridge and lasso), polynomial and spline regression models comparing the adjusted R-squared the p-values, and used 5-fold CV to compare the top model to determine which model had the best prediction accuracy while working on new, unseen data. 

After multiple models were compared from their adjusted R-squared and 5-fold CV, the linear model with all of the interaction terms was selected based on the balance between predictive accuracy (RMSE = 2.01, Adjusted R-squared = 0.74). The interaction terms were included to capture the potential effects of predictors coupled together. The linear model is interpretable and less risky to overfit, which is helpful in our goal to predict the standard deviations of the distributions given St, Re, and Fr. 

$$
Fr_{logistic} = \frac{1}{1+exp(-Fr)} \\
log(\hat{sd}) = 26.5 + 0.642 log(St) - 4.915 log(Re) -20.704 Fr_{logistic} -0.027log(St)log(Re) - 0.102log(St)Fr_{logistic}+3.557log(Re)Fr_{logistic}
$$

```{r}
model_summary <- data.frame(
  Model = c(
    "lin","lin_interactions","lin_interactions_subset","poly","spline","lasso",
    "ridge"
  ),
  RMSE = c(2.077942, 2.013816, 2.166138, 2.072878,2.115911,2.068582, 2.080013)
)

kable(model_summary)

```
 
When running 5-fold CV, the RMSE was 2.0138, which lower compared to other linear, ridge, lasso, spline and polynomial models.


```{r}
#COULD DELETE THIS MODEL
lm.fit <- lm(log(sd) ~ (log_St+log_Re+logistic_Fr)^2, data)
#plot(lm.fit) 
summary(lm.fit)
```



## Skew

Looking at the distribution of a Skew through the EDA and Box-Cox transformation (below), a square root transformation on Skew was deemed most appropriate to address the right skewness of the response variable.

```{r}
# EDA for skew
p1 <- ggplot(data, aes(x = skewness)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Skew", x = "Skew", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(skewness ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The model which best fit the sqrt(skewness) response is a ridge model fit with 5 fold cross validation. To evaluate the significance of coefficients, the same model was then fit with OLS to develop better intuition regarding the magnitude and significance of coefficients. The below table highlights two of the most significant coefficients:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_sig <-
  data.frame(
    Predictor = c("logistic_Fr", "Re:logistic_Fr"),
    Estimate = c(-21.35, 0.055)
  )
kable(skew_sig)
```
NOTE: Need to add interpretation of the coefficients here

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model. 

```{r, fig.width=9, fig.height=3}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```


# Results

## Standard Deviation



CONFIDENCE INTERVAL

```{r}
summary(lm.fit)
```


From our model, we can see that log_Re, logistic_Fr, and the interaction between log_Re and logistic_Fr has significant effects on the standard deviation of the particle cluster volume distribution. For example, holding other factors constant, for a 1% increase in the Reynold's Number, there would be a 4.915% in decrease standard deviation. Also, holding other factors constant, for a 1% increase in the Stokes number, there would be a 20.704% decrease in standard deviation. In context, this means that more turbulent regimes and regimes with more dense particles lead to narrower particle cluster distributions. However, the effect with Reynold's Number is reduces in regimes with higher gravitational acceleration, or Fr, as seen with the interaction effect between log(Re) and logisitc(Fr). 




Predictions on Test Data

```{r}
test <- read.csv("Data/data-test.csv")

test <- test |>
  mutate(logistic_Fr = 1/(1+exp(-Fr)))


sd_fit <- lm(log(sd) ~ (log(St)+log(Re)+logistic_Fr)^2, data)

sd_predict <- predict(sd_fit, test)
test <- test |>
  mutate(sd = sd_predict)

test
```


# Conclusion
