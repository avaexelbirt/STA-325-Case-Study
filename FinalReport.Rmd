---
title: "STA 325 Case Study"
author: "Abby L., Ava E., Ella T., Grady P. Laura C."
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r, echo=FALSE, message=FALSE}
# Loading packages and data
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)
library(knitr)
library(patchwork)

# Loading the data
data <- read.csv("Data/data_transformed.csv")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction

Whether it’s stirring a morning cup of coffee or feeling your stomach lurch during a bumpy airplane ride, nearly everyone has experienced some form of turbulence. Beyond these everyday encounters, turbulence plays a crucial role in many complex natural and industrial processes, from air pollution and chemical reactions to heat transfer and weather systems. Despite its ubiquity, turbulence remains notoriously difficult to predict and has long been regarded as “the last great unsolved problem in classical physics.”

While we may not be physicists ourselves, as members of Stat 325, we have undertaken the challenge of developing a predictive model to better understand this elusive phenomenon in collaboration with our Professor, Simon Mak. Given observations for fluid turbulence (quantified by Reynolds number Re), gravitational acceleration (quantified by Froud number Fr), and the particle's characteristics (quantified by Stokes number St), we have explored models designed to predict the mean, standard deviation, skewness, and kurtosis of the spatial distribution and clustering of particles in clouds in a state of idealized turbulence. Our goal is to create a model that balances predictive accuracy with interpretability; one that not only performs well statistically but also provides clear, meaningful insights into how different conditions influence turbulent behavior.

# Methodology

Observing the distributions of the predictor variables, the square root of St was used as well as the log of Re to improve the symmetry of the distribution to address the constant variance assumption. Additionally, the logit function was applied to Fr so that all values are bound between 0 and 1.

One concern with the data is that Re and Fr each only have 3 unique values. Hence, using these variables to model the distribution of the response variable requires a large amount of uncertainty and interpolation at values of these variables between the 3 distinct values in the data. To continue refining the models, additional data with different values for these models will reduce uncertainty and improve model fit.

## Mean

To predict the mean particle cluster volume, we assessed the performance of a few different models including simple linear regression, interaction terms, polynomial models, ridge regression, and spline. We then assessed the fit of each by performing 5-fold CV. The polynomial model performed best. Then, we performed a log transform on the response variable. Our final model is the polynomial regression model on the log-transformed mean. The log transformation was necessary because without it, the residuals showed clear patterns and heteroscedasticity. After transformation, the residuals appear to be randomly scattered around zero with constant variance. The log transformation also addressed the right-skewed distribution of mean cluster volumes in the raw data.

$$
\log(\text{mean}) = \beta_0 
+ \beta_1 \, \text{poly}_1(\sqrt{St}) 
+ \beta_2 \, \text{poly}_2(\sqrt{St}) 
+ \beta_3 \, \text{poly}_1(Re) 
+ \beta_4 \, \text{poly}_2(Re) 
+ \beta_5 \, \text{poly}_1(\text{logisticFr}) 
+ \beta_6 \, \text{poly}_2(\text{logisticFr}) 
+ \beta_7 \, (\sqrt{St} \cdot Re) 
+ \beta_8 \, (\sqrt{St} \cdot \text{logisticFr}) 
+ \beta_9 \, (Re \cdot \text{logisticFr}) 
$$ The final model includes second-degree polynomial terms for all three predictors (√St, Re, and the logistic-transformed Fr) along with three two-way interactions: √St:Re, √St:logistic_Fr, and Re:logistic_Fr. This model achieved great fit, with an adjusted R² of 0.9978, meaning it explains over 99% of the variation in log(mean) cluster volume. The model's RMSE of 0.098 on the log scale translates to predictions that are typically within about 10% of the true mean cluster volume when back-transformed, although there are still some uncertainty regarding the predictions due to the presence of categorical predictors.

We also ran ANOVA which reveals which predictors contribute most substantially to explaining mean cluster volume. The results show that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19). All three interaction terms were statistically significant (p \< 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p \< 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions. To furhter assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

```{r, echo = FALSE}
mean_model <- lm(log(mean) ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
    sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, data= data)

# new df that varies Re while fixing St and Fr at their means
newdata_mean <- with(data, data.frame(
  Re          = seq(min(Re, na.rm = TRUE), max(Re, na.rm = TRUE), length.out = 100),
  St          = rep(mean(St, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for Re values
pred_meanRe <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanRe <- cbind(newdata_mean, pred_meanRe)

plot_meanRe1 <- ggplot(plot_meanRe, aes(x= Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(mean))",
    x = "Re",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies St while fixing Re and Fr at their means
newdata_mean <- with(data, data.frame(
  St          = seq(min(St, na.rm = TRUE), max(St, na.rm = TRUE), length.out = 100),
  Re          = rep(mean(Re, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for St values
pred_meanSt <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanSt <- cbind(newdata_mean, pred_meanSt)

plot_meanSt1 <- ggplot(plot_meanSt, aes(x= St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "St",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies Fr while fixing Re and St at their means
newdata_mean <- with(data, data.frame(
  logistic_Fr = seq(min(data$logistic_Fr, na.rm = TRUE), max(data$logistic_Fr, na.rm = TRUE), length.out = 100),
  Re  = rep(mean(Re, na.rm = TRUE), 100),
  St = rep(mean(St, na.rm = TRUE), 100)
))

# 95% CI for Fr values
pred_meanFr <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanFr <- cbind(newdata_mean, pred_meanFr)

plot_meanFr1 <- ggplot(plot_meanFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic(Fr)",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

```

```{r, fig.width=9, fig.height=3}
(plot_meanRe1 | plot_meanSt1 | plot_meanFr1)
```

The plots depict the 95% CI for the predicted log(mean) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are very confident that there appears to be a strong negative relationship between Re and log(mean). For St, we are fairly confident that there is a nonlinear and positive relationship between St and log(mean). However, the wide confidence intervals at the edges depict some areas of uncertainty. For Fr, we have some confidence that the relationship between logistic(Fr) and log(mean) is U-shaped, such that at low and high values, log(mean)is higher and values in the middle are lower. There appears to be some uncertainty near the curvature, but the pattern appears to be overall evident of a non-monotonic effect of logistic_Fr on log(mean). 

## Standard Deviation

```{r}
# EDA for standard deviation
p1 <- ggplot(data, aes(x = sd)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(sd ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The Box Cox transformation shows that the optimal lambda is around 0, so the log transformation on sd is the optimal transformation to address skewness of the response variable.

We were given a training and testing dataset, so I trained different linear (with and without ridge and lasso), polynomial and spline regression models comparing the adjusted R-squared the p-values, and used 5-fold CV to compare the top model to determine which model had the best prediction accuracy while working on new, unseen data.

After multiple models were compared from their adjusted R-squared and 5-fold CV, the linear model with all of the interaction terms was selected based on the balance between predictive accuracy (RMSE = 2.01, Adjusted R-squared = 0.74). The interaction terms were included to capture the potential effects of predictors coupled together. The linear model is interpretable and less risky to overfit, which is helpful in our goal to predict the standard deviations of the distributions given St, Re, and Fr.

$$
Fr_{logistic} = \frac{1}{1+exp(-Fr)} \\
log(\hat{sd}) = 26.5 + 0.642 log(St) - 4.915 log(Re) -20.704 Fr_{logistic} -0.027log(St)log(Re) - 0.102log(St)Fr_{logistic}+3.557log(Re)Fr_{logistic}
$$

```{r}
model_summary <- data.frame(
  Model = c(
    "lin","lin_interactions","lin_interactions_subset","poly","spline","lasso",
    "ridge"
  ),
  RMSE = c(2.077942, 2.013816, 2.166138, 2.072878,2.115911,2.068582, 2.080013)
)

kable(model_summary)

```

When running 5-fold CV, the RMSE for the selected model is 2.0138, which is lower compared to other linear, ridge, lasso, spline and polynomial models.

```{r, echo = FALSE}
#COULD DELETE THIS MODEL
lm.fit <- lm(log(sd) ~ (log_St+log_Re+logistic_Fr)^2, data)
#plot(lm.fit) 
# summary(lm.fit)
```

```{r, echo= FALSE}
sd_model <- lm(log(sd) ~ log_St + log_Re + logistic_Fr +
              log_St:log_Re + log_St:logistic_Fr + log_Re:logistic_Fr,
              data = data)
```

```{r, echo = FALSE}
newdata_sd <- data.frame(
  log_St = seq(min(data$log_St), max(data$log_St), length.out = 100),
  log_Re = mean(data$log_Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdSt <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdSt <- cbind(newdata_sd, pred_sdSt)

plot_sdSt1 <- ggplot(plot_sdSt, aes(x= log_St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(SD))",
    x = "log(St)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  log_Re = seq(min(data$log_Re), max(data$log_Re), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdRe <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdRe <- cbind(newdata_sd, pred_sdRe)

plot_sdRe1 <- ggplot(plot_sdRe, aes(x= log_Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "log(Re)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  log_Re = mean(data$log_Re, na.rm = TRUE)
)

pred_sdFr <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdFr <- cbind(newdata_sd, pred_sdFr)

plot_sdFr1 <- ggplot(plot_sdFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic(Fr)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()
```

```{r, fig.width=9, fig.height=3}
(plot_sdSt1 | plot_sdRe1 | plot_sdFr1)
```
The plots depict the 95% CI for the predicted log(sd) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are fairly confident that there appears to be a positive relationship between log(St) and log(SD), with greater variability near the extremes. For Re, we are very confident that there is a strong negative relationship between log(Re) and log(SD), such that at larger Re values, there is less variability in log(Re). For Fr, we have some confidence that the relationship between logistic(Fr) and log(SD) is negative; however, the confidence band is much wider compared to the other predictors, which means that there does appear to be more uncertainty. 

## Skew

Looking at the distribution of a Skew and a Box-Cox transformation, the square root transformation is selected to address the skewness of the response variable.

```{r, fig.height=3, fig.width=2}
# EDA for skew
p1 <- ggplot(data, aes(x = skewness)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Skew", x = "Skew", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(skewness ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The model which best fit the √(Skew) response is a ridge model fit using 5-fold cross validation. To evaluate the significance of coefficients, the same predictors were fit with OLS to develop better intuition regarding the magnitude and significance of coefficients.

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_sig <-
  data.frame(
    Predictor = c("logistic_Fr", "Re:logistic_Fr"),
    Estimate = c(-21.35, 0.055)
  )
kable(skew_sig)
```

From the OLS model, the above coefficients were determined to be the most significant:

-   Transforming logistic_Fr back to Fr yields a coefficient extremely close to 0. This indicates that Fr is not extremely significant for predicting the distribution of √(Skew)

-   Although Fr by itself has a basically negligible impact on the distribution of √(Skew), the interaction between Fr and Re is statistically significant. Lower-hanging clouds have a higher Fr value so a positive interaction between Re and logistic_Fr indicates that either as a cloud's Fr increases (the cloud get lower) or Re increases (the flow is more turbulent), √(Skew) is increasing. Given that logistic_Fr has a negative coefficient yet the interaction between Re and logistic_Fr is positive, exploring the relationship between the height of the cloud and the turbulence of the flow warrants further investigation.

In a separate model with the same linear combinations of predictors, logistic_Fr and Re were also treated categorically. Using categorical variables greatly improved the model performance (quantified by $R^2$, AIC, BIC, and Cp). Despite a categorical model being an unrealistic fit if new data has values which don't fall into these categories, the model still provided insights into the model fit:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_cat <-
  data.frame(
    Predictor = c("sqrt(St):as.factor(Re)224", "sqrt(St):as.factor(Re)398  "),
    Estimate = c(-1.4173, -2.5683)
  )
kable(skew_cat)
```

-   

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model.

```{r, fig.width=9, fig.height=3}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```

# Results

## Standard Deviation

CONFIDENCE INTERVAL

```{r}
summary(lm.fit)
```

From our model, we can see that log_Re, logistic_Fr, and the interaction between log_Re and logistic_Fr has significant effects on the standard deviation of the particle cluster volume distribution. For example, holding other factors constant, for a 1% increase in the Reynold's Number, there would be a 4.915% in decrease standard deviation. Also, holding other factors constant, for a 1% increase in the Stokes number, there would be a 20.704% decrease in standard deviation. In context, this means that more turbulent regimes and regimes with more dense particles lead to narrower particle cluster distributions. However, the effect with Reynold's Number is reduces in regimes with higher gravitational acceleration, or Fr, as seen with the interaction effect between log(Re) and logisitc(Fr).

Predictions on Test Data

```{r}
test <- read.csv("Data/data-test.csv")

test <- test |>
  mutate(logistic_Fr = 1/(1+exp(-Fr)))


sd_fit <- lm(log(sd) ~ (log(St)+log(Re)+logistic_Fr)^2, data)

sd_predict <- predict(sd_fit, test)
test <- test |>
  mutate(sd = sd_predict)

test
```

# Conclusion
