---
title: "STA 325 Case Study"
author: "Abby L., Ava E., Ella T., Grady P. Laura C."
output: pdf_document
---

```{r, echo=FALSE, message=FALSE}
# Loading packages and data
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)
library(knitr)
library(patchwork)

# Loading the data
data <- read.csv("~/STA-325-Case-Study/Individual Case Study First Steps/data_transformed.csv")
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

```

```{r}
## Loading the data
data_csv <- read.csv("Data/data-train.csv")
data_test <- read.csv("Data/data-test.csv")
# convert raw moments to central moments
library(moments)

# raw moments from training data
m1 <- data_csv$R_moment_1   
m2 <- data_csv$R_moment_2
m3 <- data_csv$R_moment_3
m4 <- data_csv$R_moment_4

# raw moments starting at 0
raw_mat0 <- cbind(m0 = 1, m1 = m1, m2 = m2, m3 = m3, m4 = m4)

# raw to central
central_all <- t(apply(raw_mat0, 1, raw2central))



output <- data.frame(
  mean        = m1, # mean- first raw moment
  mu2_central = central_all[, 3], # sd 
  mu3_central = central_all[, 4], # skew
  mu4_central = central_all[, 5] # kurtosis
)

# Converting the central moments to statistics by dividing by sd
output$sd <- sqrt(output$mu2_central) # sd= sqrt(2nd central moment)
output$skewness <- output$mu3_central / (output$sd^3) # 3rd cm / sd^3
output$kurtosis <- output$mu4_central / (output$sd^4) #4th cm/ sd^4

central_moments_combined <- cbind(data_csv, output)
```

# Transform Predictors

```{r}
# Final dataframe with transformed predictors as well
data <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logitistic transformation for inf
         )

data_test <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logitistic transformation for inf
         )
```


# Introduction

Whether it’s stirring a morning cup of coffee or feeling your stomach lurch during a bumpy airplane ride, nearly everyone has experienced some form of turbulence. Beyond these everyday encounters, turbulence plays a crucial role in many complex natural and industrial processes, from air pollution and chemical reactions to heat transfer and weather systems. Despite its ubiquity, turbulence remains notoriously difficult to predict and has long been regarded as “the last great unsolved problem in classical physics.”

While we may not be physicists ourselves, as members of Stat 325, we have undertaken the challenge of developing a predictive model to better understand this elusive phenomenon in collaboration with our Professor, Simon Mak. Specifically, we have explored models designed to predict the mean, standard deviation, skewness, and kurtosis of turbulence. Our goal is to create a model that balances predictive accuracy with interpretability; one that not only performs well statistically but also provides clear, meaningful insights into how different conditions influence turbulent behavior.


### Exploratory Data Analysis

```{r}

data_long <- data %>%
  dplyr::select(Fr, Re, St) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  mutate(Transformation = "Original") %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), sqrt)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Sqrt")
  ) %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), log)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Log")
  )
ggplot(data_long, aes(x = Value, fill = Transformation)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_grid(Variable ~ Transformation, scales = "free") +
  scale_fill_manual(values = c("Original" = "skyblue", 
                               "Sqrt" = "lightgreen", 
                               "Log" = "salmon")) +
  labs(title = "Histograms of Fr, Re, St (Original, Sqrt, Log)",
       x = "Value", y = "Count") +
  theme_minimal()

```


```{r}

library(corrplot)

# Select relevant variables from your dataset
vars <- data[, c("Re", "St_sqrt", "logistic_Fr", 
                 "R_moment_1", "R_moment_2", "R_moment_3", "R_moment_4")]

# Compute correlation matrix
corr_matrix <- cor(vars, use = "complete.obs")  # handles missing values

# Plot correlation matrix
corrplot(corr_matrix, method = "color", 
         type = "upper", 
         addCoef.col = "black",  # show correlation coefficients
         tl.col = "black",       # axis labels color
         tl.srt = 45)            # label rotation
```







We observed that almost all of the residual models have clustered residuals, likely due to both Fr and Re only having 3 unique levels in the data.


# Methodology

## Skew

Looking at the distribution of a Skew through the EDA and Box-Cox transformation (below), a square root transformation on Skew was deemed most appropriate to address the right skewness of the response variable.

```{r}
# EDA for skew
p1 <- ggplot(data, aes(x = skewness)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Skew", x = "Skew", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(skewness ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The model which best fit the sqrt(skewness) response is a ridge model fit with 5 fold cross validation. To evaluate the significance of coefficients, the same model was then fit with OLS to develop better intuition regarding the magnitude and significance of coefficients. The below table highlights two of the most significant coefficients:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_sig <-
  data.frame(
    Predictor = c("logistic_Fr", "Re:logistic_Fr"),
    Estimate = c(-21.35, 0.055)
  )
kable(skew_sig)
```
NOTE: Need to add interpretation of the coefficients here

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model. 

```{r, fig.width=9, fig.height=3}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```


# Results


# Conclusion
