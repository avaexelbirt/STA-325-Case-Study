---
title: "STA 325 Case Study"
author: "Abby L., Ava E., Ella T., Grady P. Laura C."
output: pdf_document
---

```{r, echo=FALSE, message=FALSE}
# Loading packages and data
library(tidyverse)
library(tidyr)
library(dplyr)
library(ggplot2)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)
library(knitr)
library(patchwork)

# Loading the data
data <- read.csv("Data/data_transformed.csv")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

```

```{r}
## Loading the data
data_csv <- read.csv("Data/data-train.csv")
data_test <- read.csv("Data/data-test.csv")
# convert raw moments to central moments
library(moments)

# raw moments from training data
m1 <- data_csv$R_moment_1   
m2 <- data_csv$R_moment_2
m3 <- data_csv$R_moment_3
m4 <- data_csv$R_moment_4

# raw moments starting at 0
raw_mat0 <- cbind(m0 = 1, m1 = m1, m2 = m2, m3 = m3, m4 = m4)

# raw to central
central_all <- t(apply(raw_mat0, 1, raw2central))



output <- data.frame(
  mean        = m1, # mean- first raw moment
  mu2_central = central_all[, 3], # sd 
  mu3_central = central_all[, 4], # skew
  mu4_central = central_all[, 5] # kurtosis
)

# Converting the central moments to statistics by dividing by sd
output$sd <- sqrt(output$mu2_central) # sd= sqrt(2nd central moment)
output$skewness <- output$mu3_central / (output$sd^3) # 3rd cm / sd^3
output$kurtosis <- output$mu4_central / (output$sd^4) #4th cm/ sd^4

central_moments_combined <- cbind(data_csv, output)
```

# Transform Predictors

```{r}
# Final dataframe with transformed predictors as well
data <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logistic transformation for inf
         )

data_test <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logistic transformation for inf
         )
```


# Introduction

Whether it’s stirring a morning cup of coffee or feeling your stomach lurch during a bumpy airplane ride, nearly everyone has experienced some form of turbulence. Beyond these everyday encounters, turbulence plays a crucial role in many complex natural and industrial processes, from air pollution and chemical reactions to heat transfer and weather systems. Despite its ubiquity, turbulence remains notoriously difficult to predict and has long been regarded as “the last great unsolved problem in classical physics.”

While we may not be physicists ourselves, as members of Stat 325, we have undertaken the challenge of developing a predictive model to better understand this elusive phenomenon in collaboration with our Professor, Simon Mak. Specifically, we have explored models designed to predict the mean, standard deviation, skewness, and kurtosis of turbulence. Our goal is to create a model that balances predictive accuracy with interpretability; one that not only performs well statistically but also provides clear, meaningful insights into how different conditions influence turbulent behavior.


### Exploratory Data Analysis

```{r}

data_long <- data %>%
  dplyr::select(Fr, Re, St) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  mutate(Transformation = "Original") %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), sqrt)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Sqrt")
  ) %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), log)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Log")
  )
ggplot(data_long, aes(x = Value, fill = Transformation)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_grid(Variable ~ Transformation, scales = "free") +
  scale_fill_manual(values = c("Original" = "skyblue", 
                               "Sqrt" = "lightgreen", 
                               "Log" = "salmon")) +
  labs(title = "Histograms of Fr, Re, St (Original, Sqrt, Log)",
       x = "Value", y = "Count") +
  theme_minimal()

```


```{r}

library(corrplot)

# Select relevant variables from your dataset
vars <- data[, c("Re", "St_sqrt", "logistic_Fr", 
                 "R_moment_1", "R_moment_2", "R_moment_3", "R_moment_4")]

# Compute correlation matrix
corr_matrix <- cor(vars, use = "complete.obs")  # handles missing values

# Plot correlation matrix
corrplot(corr_matrix, method = "color", 
         type = "upper", 
         addCoef.col = "black",  # show correlation coefficients
         tl.col = "black",       # axis labels color
         tl.srt = 45)            # label rotation
```







We observed that almost all of the residual models have clustered residuals, likely due to both Fr and Re only having 3 unique levels in the data.


# Methodology

The EDA shows a more symmetric spread for log(St) and log(Re), so we log-transformed predictors St and Re to satisfy the constant variance assumption necessary. Given that Fr has "Inf" values, we used the logistic function to transform the predictor so that all the values were finite, where they all ended up being between 0 and 1. 

## Mean 
To predict the mean particle cluster volume, we assessed the performance of a few different models including simple linear regression, interaction terms, polynomial models, ridge regression, and spline. We then assessed the fit of each by performing 5-fold CV. The polynomial model performed best. Then, we performed a log transform on the response variable. Our final model is the polynomial regression model on the log-transformed mean. The log transformation was necessary because without it, the residuals showed clear patterns and heteroscedasticity. After transformation, the residuals appear to be randomly scattered around zero with constant variance. The log transformation also addressed the right-skewed distribution of mean cluster volumes in the raw data.
   
$$
\log(\text{mean}) = \beta_0 
+ \beta_1 \, \text{poly}_1(\sqrt{St}) 
+ \beta_2 \, \text{poly}_2(\sqrt{St}) 
+ \beta_3 \, \text{poly}_1(Re) 
+ \beta_4 \, \text{poly}_2(Re) 
+ \beta_5 \, \text{poly}_1(\text{logisticFr}) 
+ \beta_6 \, \text{poly}_2(\text{logisticFr}) 
+ \beta_7 \, (\sqrt{St} \cdot Re) 
+ \beta_8 \, (\sqrt{St} \cdot \text{logisticFr}) 
+ \beta_9 \, (Re \cdot \text{logisticFr}) 
$$
The final model includes second-degree polynomial terms for all three predictors (√St, Re, and the logistic-transformed Fr) along with three two-way interactions: √St:Re, √St:logistic_Fr, and Re:logistic_Fr. This model achieved great fit, with an adjusted R² of 0.9978, meaning it explains over 99% of the variation in log(mean) cluster volume. The model's RMSE of 0.098 on the log scale translates to predictions that are typically within about 10% of the true mean cluster volume when back-transformed, although there are still some uncertainty regarding the predictions due to the presence of categorical predictors. 

We also ran ANOVA which reveals which predictors contribute most substantially to explaining mean cluster volume. The results show that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19). All three interaction terms were statistically significant (p \< 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p \< 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions. To furhter assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

## Standard Deviation

```{r}
# EDA for standard deviation
p1 <- ggplot(data, aes(x = sd)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(sd ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```
The Box Cox transformation shows that the optimal lambda is around 0, so the log transformation on sd is the optimal transformation to address skewness of the response variable. 

We were given a training and testing dataset, so I trained  different linear (with and without ridge and lasso), polynomial and spline regression models comparing the adjusted R-squared the p-values, and used 5-fold CV to compare the top model to determine which model had the best prediction accuracy while working on new, unseen data. 

After multiple models were compared from their adjusted R-squared and 5-fold CV, the linear model with all of the interaction terms was selected based on the balance between predictive accuracy (RMSE = 2.01, Adjusted R-squared = 0.74). The interaction terms were included to capture the potential effects of predictors coupled together. The linear model is interpretable and less risky to overfit, which is helpful in our goal to predict the standard deviations of the distributions given St, Re, and Fr. 

$$
Fr_{logistic} = \frac{1}{1+exp(-Fr)} \\
log(\hat{sd}) = 26.5 + 0.642 log(St) - 4.915 log(Re) -20.704 Fr_{logistic} -0.027log(St)log(Re) - 0.102log(St)Fr_{logistic}+3.557log(Re)Fr_{logistic}
$$

```{r}
model_summary <- data.frame(
  Model = c(
    "lin","lin_interactions","lin_interactions_subset","poly","spline","lasso",
    "ridge"
  ),
  RMSE = c(2.077942, 2.013816, 2.166138, 2.072878,2.115911,2.068582, 2.080013)
)

kable(model_summary)

```
 
When running 5-fold CV, the RMSE was 2.0138, which lower compared to other linear, ridge, lasso, spline and polynomial models.


```{r}
#COULD DELETE THIS MODEL
lm.fit <- lm(log(sd) ~ (log_St+log_Re+logistic_Fr)^2, data)
#plot(lm.fit) 
summary(lm.fit)
```



## Skew

Looking at the distribution of a Skew through the EDA and Box-Cox transformation (below), a square root transformation on Skew was deemed most appropriate to address the right skewness of the response variable.

```{r}
# EDA for skew
p1 <- ggplot(data, aes(x = skewness)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Skew", x = "Skew", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(skewness ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

The model which best fit the sqrt(skewness) response is a ridge model fit with 5 fold cross validation. To evaluate the significance of coefficients, the same model was then fit with OLS to develop better intuition regarding the magnitude and significance of coefficients. The below table highlights two of the most significant coefficients:

```{r, echo=FALSE, message=FALSE}
# Table with the significant coefficients
skew_sig <-
  data.frame(
    Predictor = c("logistic_Fr", "Re:logistic_Fr"),
    Estimate = c(-21.35, 0.055)
  )
kable(skew_sig)
```
NOTE: Need to add interpretation of the coefficients here

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model. 

```{r, fig.width=9, fig.height=3}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```


# Results

## Standard Deviation



CONFIDENCE INTERVAL

```{r}
summary(lm.fit)
```


From our model, we can see that log_Re, logistic_Fr, and the interaction between log_Re and logistic_Fr has significant effects on the standard deviation of the particle cluster volume distribution. For example, holding other factors constant, for a 1% increase in the Reynold's Number, there would be a 4.915% in decrease standard deviation. Also, holding other factors constant, for a 1% increase in the Stokes number, there would be a 20.704% decrease in standard deviation. In context, this means that more turbulent regimes and regimes with more dense particles lead to narrower particle cluster distributions. However, the effect with Reynold's Number is reduces in regimes with higher gravitational acceleration, or Fr, as seen with the interaction effect between log(Re) and logisitc(Fr). 




Predictions on Test Data

```{r}
test <- read.csv("Data/data-test.csv")

test <- test |>
  mutate(logistic_Fr = 1/(1+exp(-Fr)))


sd_fit <- lm(log(sd) ~ (log(St)+log(Re)+logistic_Fr)^2, data)

sd_predict <- predict(sd_fit, test)
test <- test |>
  mutate(sd = sd_predict)

test
```


# Conclusion
