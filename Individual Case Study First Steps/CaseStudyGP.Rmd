---
title: "STA 325 Case Study"
author: "Grady Purcell"
output: html_document
---

Description: This is my original work for the case study EDA section of the analysis (before 10/16 meeting).

```{r}
## Loading the libraries 
library(tidyverse)
library(dplyr)
library(ggplot2)
# install.packages(moments)
library(moments)
library(glmnet)
library(carets) # create k-folds
library(splines)

## Loading the data
data <- read.csv("data_transformed.csv")

```

## EDA for Summary Statistics

### Mean
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = mean)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Mean", x = "Mean", y = "Count")

# log transformed
ggplot(data, aes(x = log(mean))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Mean)", x = "log(Mean)", y = "Count")

# squared transformed
ggplot(data, aes(x = mean^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Mean^2", x = "Mean^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(mean))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Mean)", x = "sqrt(Mean)", y = "Count")

```

Mean is very right-skewed with the majority of the means concentrated around 0. log(Mean) does make the distribution more symmetric, but converts all of the values to negative which is less interpretable. 

### Standard Deviation
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = sd)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count")

# log transformed
ggplot(data, aes(x = log(sd))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Standard Deviation)", x = "log(Standard Deviation)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = sd^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Standard Deviation^2", x = "Standard Deviation^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(sd))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Standard Deviation)", x = "sqrt(Standard Deviation)", 
       y = "Count")

```

We notice a similar pattern in that Standard Deviation is also very right skewed with the majority of the values close to 0. Log transforming Standard Deviation does make the distribution more symmetric, but also negative which is less interpretable.

### Skewness
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = skewness)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Skew", x = "Skew", y = "Count")

# log transformed
ggplot(data, aes(x = log(skewness))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Skew)", x = "log(Skew)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = skewness^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Skew^2", x = "Skew^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(skewness))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Skew)", x = "sqrt(Skew)", 
       y = "Count")
```

Skew has a bi-modal distribution with there being very few observations with a skew between 100 and 200. None of the transformations appear to improve the distribution of skew, often making the distribution tri-modal with clusters instead of more symmetric. For this reason, it might be best to stick with the untransformed values for skew.

### Kurtosis
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = kurtosis)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Kurtosis", x = "Kurtosis", y = "Count")

# log transformed
ggplot(data, aes(x = log(kurtosis))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Kurtosis)", x = "log(Kurtosis)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = kurtosis^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Kurtosis^2", x = "Kurtosis^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(kurtosis))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Kurtosis)", x = "sqrt(Kurtosis)", 
       y = "Count")
```

Kurtosis is also right-skewed but not as drastically as mean and standard deviation above. Compared to the transformed distributions for Kurtosis, the untransformed distribution is more symmetric which would indicate that keeping kurtosis untransformed might yield the best results. 

# Skew

## Linear Regression Model
```{r}
# Linear regression model

lm_skew <- lm(skewness ~ St + Re + logistic_Fr, data = data)
summary(lm_skew)
```

$R^2$ = 0.4107 is low with only 41.07 % of the variability in the skewness explained by the predictors. All coefficients except St are significant past the 1% level.

```{r}
par(mfrow = c(2,2))
plot(lm_skew)
```

The model diagnostic plots raise a lot of concerns about the fit of the linear model. Firstly, there is distinct clustering for the residuals vs fitted values when we would expect to see randomness. The Q-Q plot also has some distinct patterns which is nt ideal. There are also several high leverage points which indicate they are on average very far from the initial predictors. 

```{r}
# Linear model with the best transformation from each based on EDA
# RESULTS: performs worse than the model above
lm_skew2 <- lm(skewness ~ St_sqrt + Re + logistic_Fr, data = data)
'''
summary(lm_skew2)
par(mfrow = c(2,2))
plot(lm_skew2)
'''
```


## Linear Model with Interaction Terms
```{r}
# Interacting Re and logistic_Fr
lm_skew_int1 <- lm(skewness ~ St + Re * logistic_Fr, data = data)
summary(lm_skew_int1)
```
This model has a higher adjusted $R^2$ value yet St and Re are both now insignificant.

```{r}
par(mfrow = c(2,2))
plot(lm_skew_int1)
```
We still notice the clustering patterns in the residual which are concerning and some points now have even higher leverage.

```{r}
# Interacting St and Re
# RESULTS: performs worse than int1 above with extreme high leverage points
lm_skew_int2 <- lm(skewness ~ St * Re + logistic_Fr, data = data)
#summary(lm_skew_int2)
#par(mfrow = c(2,2))
#plot(lm_skew_int2)

# Interacting St and Re
# RESULTS: performs worse than int1 above with extreme high leverage points
lm_skew_int3 <- lm(skewness ~ Re + St * logistic_Fr, data = data)
#summary(lm_skew_int3)
#par(mfrow = c(2,2))
#plot(lm_skew_int3)

# Interacting all three variables
# RESULTS: R^2 is slightly worse than int1 above with extreme high leverage points
lm_skew_int4 <- lm(skewness ~ Re * St * logistic_Fr, data = data)
#summary(lm_skew_int4)
#par(mfrow = c(2,2))
#plot(lm_skew_int4)
```

## Polynomial Models
```{r}
# Polynomials of degree 2 for St and Re
# RESULTS: adjusted R^2 is worse than interaction model above and residuals are still clustered, 
# but the Q-Q plot is better, St is still insignificant
poly_skew1 <- lm(skewness ~ poly(St, 2) + poly(Re, 2) + logistic_Fr, data = data)
#summary(poly_skew1)
#par(mfrow = c(2,2))
#plot(poly_skew1)


# Polynomials of degree 3 for St, 2 for Re
# RESULTS: adjusted R^2 is slightly worse than model above, St is still insignificant
poly_skew2 <- lm(skewness ~ poly(St, 3) + poly(Re, 2) + logistic_Fr, data = data)
#summary(poly_skew2)
#par(mfrow = c(2,2))
#plot(poly_skew2)
```
## Transforming the Input Variables
```{r}
# Treating the Fr and Re as categorical
lm_skew_tr_inp1 <- lm(skewness ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_skew_tr_inp1)
par(mfrow = c(2,2))
plot(lm_skew_tr_inp1)
```

Using Fr and Re as categorical yields a much higher adjusted R^2 values. The residuals are still clustered but it doesn't appear as drastic and the Q-Q plot is better. However, all points have a relatively high Cook's distance which is surprising and not ideal. 

## Transforming the Response Variable
```{r}
# Taking the square root of skewness
lm_skew_tr_resp1 <- lm(sqrt(skewness) ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_skew_tr_resp1)
par(mfrow = c(2,2))
plot(lm_skew_tr_resp1)
```
Taking the square root of skewness also results in a really high adjusted R^2 and slightly better model fit. The distribution of the residuals is still very clustered which is not ideal for model fit. The majority of the residuals have a Cook's distance above 0.04 which also indicates poor model performance. 

## Ridge Regression
```{r}
set.seed(325)

# Original ridge regression model

# Prepare predictors once (same rationale as above)
y <- data$skewness
X <- model.matrix(skewness ~ St + Re + logistic_Fr, data = data)[, -1]

nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```


```{r}
set.seed(325)

# Prepare predictors once (same rationale as above)
y <- data$skewness
X <- model.matrix(skewness ~ sqrt(St) + poly(Re, 2) + logistic_Fr, data = data)[, -1]


nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```

The model with the the square root transformation for St and the polynomial term for Re has a slightly lower RMSE and a much lower RMSE standard deviation. 

## Spline Model
```{r}
# Spline model with cross validation
set.seed(123)
small_data <- data |> 
  select(St, Re, logistic_Fr, skewness)
folds <- createFolds(data$skewness, k = 5)
rmse_vec <- numeric(5)
r2_vec <- numeric(5)

for (i in 1:5) {
  test_idx <- folds[[i]]
  train <- small_data[-test_idx, ]
  test  <- small_data[test_idx, ]

  fit <- lm(skewness ~ bs(St, df = 5) + bs(Re, df = 4) + 
              bs(logistic_Fr, df = 4), data = train)
  preds <- predict(fit, newdata = test)

  # Compute residuals and metrics
  y_true <- test$skewness
  n <- length(y_true)
  p <- length(coef(fit)) - 1        # exclude intercept for p

  rss <- sum((y_true - preds)^2)
  tss <- sum((y_true - mean(y_true))^2)

  r2 <- 1 - rss/tss
  adj_r2 <- 1 - (1 - r2) * ((n - 1)/(n - p - 1))

  # Store fold metrics
  rmse_vec[i]  <- sqrt(mean((y_true - preds)^2))
  r2_vec[i]    <- r2
  adjr2_vec[i] <- adj_r2
}

# Cross-validated metrics
mean_rmse  <- mean(rmse_vec)
mean_r2    <- mean(r2_vec)
mean_adj_r2 <- mean(adjr2_vec)
sd_rmse <- sd(rmse_vec)

c(CV_RMSE = mean_rmse,
  CV_R2 = mean_r2,
  CV_RMSE_sd = sd_rmse)
```

The spline model has a lower mean RMSE than the ridge regression models but a higher standard deviation for the RMSE. The R^2 is also fairly high but a better measure would probably be adjusted R^2. 


## Takeaways for Skewness:
- Best adjusted R^2 values is from using square root St and categorical values for both Re and Fr --> all coefficients are significant past the 5% level. The residuals are still clustered but it doesn't appear as drastic and the Q-Q plot is better. However, all points have a relatively high Cook's distance which is surprising and not ideal. 
- sqrt(Skewness) also yields very high adjusted R^2 values but the model diagnostic tools aren't great. 
- Ridge regression has a lower R^2 value but the RMSE is not terrible and might be worth sacrificing the R^2 if it means that we are able to do a train/test split
- The spline model has a lower mean RMSE than the ridge regression models but a higher standard deviation of RMSE. The R^2 is also fairly higher for the spline model.

# Kurtosis

## Linear Regression Model
```{r}
# Linear regression model

lm_skew <- lm(skewness ~ St + Re + logistic_Fr, data = data)
summary(lm_skew)
```




