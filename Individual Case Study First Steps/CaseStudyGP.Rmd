---
title: "STA 325 Case Study"
author: "Grady Purcell"
output: html_document
---

Description: This is my original work for the case study EDA section of the analysis (before 10/16 meeting).

```{r}
## Loading the libraries 
library(tidyverse)
library(dplyr)
library(ggplot2)
# install.packages(moments)
library(moments)
library(glmnet)
library(caret) # create k-folds
library(splines)
library(boot)
library(olsrr)
library(MASS)

## Loading the data
data <- read.csv("data_transformed.csv")

```

## Building the Final Models

### Skew

```{r}
# Boxcox transformation for skew
boxcox(lm(data$skewness ~ 1))

## Indicates that sqrt is the best transformation for skewness
```


#### Interpretable Model
```{r}
# Building the more interpetable model
m1 <- lm(sqrt(skewness) ~ sqrt(St) + as.factor(Re) + as.factor(logistic_Fr), 
         data = data)
skew_int <- summary(m1)

```

```{r}
# Getting the model diagnostics for interpretable model
cat("R²:", skew_int$r.squared, "\n")
cat("Adjusted R²:", skew_int$adj.r.squared, "\n")
cat("AIC:", AIC(m1), "\n")
cat("BIC:", BIC(m1), "\n")

# Determining the RMSE
y <- sqrt(data$skewness)
y_hat <- fitted(skew_int)
resid <- residuals(skew_int)

RMSE <- sqrt(mean(resid^2))
MSE  <- mean(resid^2)
bias <- mean(resid)

c(RMSE = RMSE, MSE = MSE, Bias = bias)

# Finding the Cp
n <- length(m1$fitted.values)
p <- length(coef(m1))       # includes intercept
SSE <- sum(resid(m1)^2)
sigma2 <- summary(m1)$sigma^2

Cp <- SSE / sigma2 - n + 2 * p
Cp

cat("Mallows' Cp:", Cp, "\n")

# Printing the anova test
anova(m1)


```
```{r}
par(mfrow = c(2, 2))
plot(m1)
```


```{r}
# Building a confidence band around the lm

# Using a model without the as.factor
m2 <- lm(sqrt(skewness) ~ sqrt(St) + Re + logistic_Fr, 
         data = data)

# Creating the sequence to make the variables continuous
St_seq <- seq(min(data$St, na.rm = TRUE),
              max(data$St, na.rm = TRUE), length.out = 100)
Re_med <- seq(min(data$Re, na.rm = TRUE),
              max(data$Re, na.rm = TRUE), length.out = 100)
log_Fr_med <- seq(min(data$logistic_Fr, na.rm = TRUE),
              max(data$logistic_Fr, na.rm = TRUE), length.out = 100)

# New dataframe with the continuous values
newdata <- data.frame(St = St_seq,
                      Re = Re_med,
                      logistic_Fr = log_Fr_med)

# Predict fitted means
pred_conf <- predict(m2, newdata, interval = "confidence")

# Combine for plotting
plot_df <- cbind(newdata, pred_conf)

## Residual plots with confidence intervals
# St plot
ggplot(plot_df, aes(x = St, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_line(aes(y = lwr), linetype = "dashed", color = "black") +
  geom_line(aes(y = upr), linetype = "dashed", color = "black") + 
  labs(title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
       x = "St", y = "Predicted sqrt(skewness)") +
  theme_minimal()

# Re plot
ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_line(aes(y = lwr), linetype = "dashed", color = "black") +
  geom_line(aes(y = upr), linetype = "dashed", color = "black") +
  geom_vline(
   xintercept = unique(data$Re),
    linetype = "dashed",
     color = "red",
     linewidth = 0.2
  ) + 
  labs(title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
       x = "Re", y = "Predicted sqrt(skewness)") +
  theme_minimal()

# Logistic_Fr plot
ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_line(aes(y = lwr), linetype = "dashed", color = "black") +
  geom_line(aes(y = upr), linetype = "dashed", color = "black") +
  geom_vline(
   xintercept = unique(data$logistic_Fr),
    linetype = "dashed",
     color = "red",
     linewidth = 0.2
  ) + 
  labs(title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
       x = "Logistic Fr", y = "Predicted sqrt(skewness)") +
  theme_minimal()
```

#### Predictive Model
```{r}
# Basic ridge model
y <- data$skewness
X <- model.matrix(sqrt(skewness) ~ sqrt(St) + Re + logistic_Fr + 
                    sqrt(St):Re + Re:logistic_Fr + sqrt(St):logistic_Fr, 
         data = data)[, -1]  

# Fitting the ridge model
ridge_fit <- cv.glmnet(X, y, alpha = 0, nfolds = 5)
```


```{r}
# Building the residual plot
# Predicted values
y_pred <- predict(ridge_fit, newx = X, s = "lambda.min")
residuals <- y - y_pred

# Residual plot
plot(y_pred, residuals, 
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residual Plot for Ridge Regression")
abline(h = 0, col = "black", lwd = 2)

# Adding a nonlinear trend to the residuals
ord <- order(y_pred)
lo <- loess(residuals ~ y_pred)
lines(y_pred[ord], predict(lo)[ord], col = "blue", lwd = 2, lty = 2)
```



```{r}
# Checking for model overfitting
set.seed(325)

y_ridge <- sqrt(data$skewness)
X_ridge <- model.matrix(~ sqrt(St) + Re + logistic_Fr + 
                    sqrt(St):Re + Re:logistic_Fr + sqrt(St):logistic_Fr, data = data)[, -1]  
n <- nrow(X)
k <- 10

# Create folds (rough random splitting)
fold_ids <- sample(rep(1:k, length.out = n))

# Initializing variables
y_true_all <- numeric(0)
y_pred_all <- numeric(0)
fold_errors <- numeric(0)

# Loop folds
for (fold in 1:k) {
  test_idx <- which(fold_ids == fold)
  train_idx <- setdiff(seq_len(n), test_idx)

  # Splitting into train and test data sets
  X_train <- X_ridge[train_idx, , drop = FALSE]
  y_train <- y_ridge[train_idx]

  X_test <- X_ridge[test_idx, , drop = FALSE]
  y_test <- y_ridge[test_idx]

  # Fit the new models
  cv_fit <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 5)   
  lambda_star <- cv_fit$lambda.min

  # Making prediction on test set
  preds <- as.numeric(predict(cv_fit, newx = X_test, s = "lambda.min"))

  # Collecting predictions results
  y_true_all <- c(y_true_all, y_test)
  y_pred_all <- c(y_pred_all, preds)
  
  # Individiual cv errors
  fold_errors[fold] <- mean((y_test - preds)^2)
}

## Model diagnostics

# R^2
SSE <- sum((y_true_all - y_pred_all)^2)
SST <- sum((y_true_all - mean(y_true_all))^2)
R2_overall <- 1 - SSE / SST

# RMSE
resid_cv <- y_true_all - y_pred_all
RMSE <- sqrt(mean((resid_cv)^2))

# Average CV MSE
mse_cv <- mean(fold_errors)
cv_sd <- sd(fold_errors)

metrics <- list(R2 = R2_overall, RMSE = RMSE, 
                SD = sd(resid))
metrics
cat("10-fold CV MSE:", mean(fold_errors), "±", sd(fold_errors), "\n")


```

### Model Diagnostics
```{r}
# Residual plot
# Predicted values
y_pred <- predict(ridge_fit, newx = X, s = "lambda.min")
residuals <- y - y_pred

# Residual plot
plot(y_pred, residuals, 
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residual Plot for Ridge Regression")
abline(h = 0, col = "red", lwd = 2)

# Adding a nonlinear trend to the residuals
ord <- order(y_pred)
lo <- loess(residuals ~ y_pred)
lines(y_pred[ord], predict(lo)[ord], col = "blue", lwd = 2, lty = 2)  
```

```{r}
# Calculating R^2 and adjusted R^2
# Calculate R^2
SSE <- sum((y - y_pred)^2)
SST <- sum((y - mean(y))^2)
R2 <- 1 - SSE/SST

# Adjusted R^2
n <- length(y)
p <- ncol(X)  # number of predictors
adj_R2 <- 1 - (1 - R2) * (n - 1) / (n - p - 1)

print(R2)
print(adj_R2)
```


```{r}
set.seed(325)

# Prepare predictors once (same rationale as above)
y <- data$skewness
X <- model.matrix(sqrt(skewness) ~ sqrt(St) + Re + logistic_Fr, 
                  data = data)[, -1]

# Using the ridge model
ridge_fit <- cv.glmnet(X, y, alpha = 0, nfolds = 10)

# Predicted values
y_pred <- predict(ridge_fit, newx = X, s = "lambda.min")
residuals <- y - y_pred


# Residual plot
plot(y_pred, residuals, 
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residual Plot for Ridge Regression")
abline(h = 0, col = "red", lwd = 2)

# Adding a nonlinear trend to the residuals
ord <- order(y_pred)
lo <- loess(residuals ~ y_pred)
lines(y_pred[ord], predict(lo)[ord], col = "blue", lwd = 2, lty = 2)  



```


```{r}
# Testing the ridge model with CV
nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res
```



## EDA for Summary Statistics

### Mean
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = mean)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Mean", x = "Mean", y = "Count")

# log transformed
ggplot(data, aes(x = log(mean))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Mean)", x = "log(Mean)", y = "Count")

# squared transformed
ggplot(data, aes(x = mean^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Mean^2", x = "Mean^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(mean))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Mean)", x = "sqrt(Mean)", y = "Count")

```

Mean is very right-skewed with the majority of the means concentrated around 0. log(Mean) does make the distribution more symmetric, but converts all of the values to negative which is less interpretable. 

### Standard Deviation
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = sd)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count")

# log transformed
ggplot(data, aes(x = log(sd))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Standard Deviation)", x = "log(Standard Deviation)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = sd^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Standard Deviation^2", x = "Standard Deviation^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(sd))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Standard Deviation)", x = "sqrt(Standard Deviation)", 
       y = "Count")

```

We notice a similar pattern in that Standard Deviation is also very right skewed with the majority of the values close to 0. Log transforming Standard Deviation does make the distribution more symmetric, but also negative which is less interpretable.

### Skewness
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = skewness)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Skew", x = "Skew", y = "Count")

# log transformed
ggplot(data, aes(x = log(skewness))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Skew)", x = "log(Skew)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = skewness^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Skew^2", x = "Skew^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(skewness))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Skew)", x = "sqrt(Skew)", 
       y = "Count")
```

Skew has a bi-modal distribution with there being very few observations with a skew between 100 and 200. None of the transformations appear to improve the distribution of skew, often making the distribution tri-modal with clusters instead of more symmetric. For this reason, it might be best to stick with the untransformed values for skew.

### Kurtosis
```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = kurtosis)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Kurtosis", x = "Kurtosis", y = "Count")

# log transformed
ggplot(data, aes(x = log(kurtosis))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Kurtosis)", x = "log(Kurtosis)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = kurtosis^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Kurtosis^2", x = "Kurtosis^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(kurtosis))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Kurtosis)", x = "sqrt(Kurtosis)", 
       y = "Count")
```

Kurtosis is also right-skewed but not as drastically as mean and standard deviation above. Compared to the transformed distributions for Kurtosis, the untransformed distribution is more symmetric which would indicate that keeping kurtosis untransformed might yield the best results. 

# Skew

## Linear Regression Model
```{r}
# Linear regression model

lm_skew <- lm(skewness ~ St + Re + logistic_Fr, data = data)
summary(lm_skew)
par(mfrow = c(2,2))
plot(lm_skew)
```

$R^2$ = 0.4107 is low with only 41.07 % of the variability in the skewness explained by the predictors. All coefficients except St are significant past the 1% level.


The model diagnostic plots raise a lot of concerns about the fit of the linear model. Firstly, there is distinct clustering for the residuals vs fitted values when we would expect to see randomness. The Q-Q plot also has some distinct patterns which is nt ideal. There are also several high leverage points which indicate they are on average very far from the initial predictors. 

```{r}
# Linear model with the best transformation from each based on EDA
# RESULTS: performs worse than the model above
lm_skew2 <- lm(skewness ~ St_sqrt + Re + logistic_Fr, data = data)
#summary(lm_skew2)
#par(mfrow = c(2,2))
#plot(lm_skew2)

```


## Linear Model with Interaction Terms
```{r}
# Interacting Re and logistic_Fr
lm_skew_int1 <- lm(skewness ~ St + Re * logistic_Fr, data = data)
summary(lm_skew_int1)
par(mfrow = c(2,2))
plot(lm_skew_int1)
```
This model has a higher adjusted $R^2$ value yet St and Re are both now insignificant.
We still notice the clustering patterns in the residual which are concerning and some points now have even higher leverage.

```{r}
# Interacting St and Re
# RESULTS: performs worse than int1 above with extreme high leverage points
lm_skew_int2 <- lm(skewness ~ St * Re + logistic_Fr, data = data)
#summary(lm_skew_int2)
#par(mfrow = c(2,2))
#plot(lm_skew_int2)

# Interacting St and Re
# RESULTS: performs worse than int1 above with extreme high leverage points
lm_skew_int3 <- lm(skewness ~ Re + St * logistic_Fr, data = data)
#summary(lm_skew_int3)
#par(mfrow = c(2,2))
#plot(lm_skew_int3)

# Interacting all three variables
# RESULTS: R^2 is slightly worse than int1 above with extreme high leverage points
lm_skew_int4 <- lm(skewness ~ Re * St * logistic_Fr, data = data)
#summary(lm_skew_int4)
#par(mfrow = c(2,2))
#plot(lm_skew_int4)
```

## Polynomial Models
```{r}
# Polynomials of degree 2 for St and Re
# RESULTS: adjusted R^2 is worse than interaction model above and residuals are still clustered, 
# but the Q-Q plot is better, St is still insignificant
poly_skew1 <- lm(skewness ~ poly(St, 2) + poly(Re, 2) + logistic_Fr, data = data)
#summary(poly_skew1)
#par(mfrow = c(2,2))
#plot(poly_skew1)


# Polynomials of degree 3 for St, 2 for Re
# RESULTS: adjusted R^2 is slightly worse than model above, St is still insignificant
poly_skew2 <- lm(skewness ~ poly(St, 3) + poly(Re, 2) + logistic_Fr, data = data)
#summary(poly_skew2)
#par(mfrow = c(2,2))
#plot(poly_skew2)
```
## Transforming the Input Variables
```{r}
# Treating the Fr and Re as categorical
lm_skew_tr_inp1 <- lm(skewness ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_skew_tr_inp1)
par(mfrow = c(2,2))
plot(lm_skew_tr_inp1)
```

Using Fr and Re as categorical yields a much higher adjusted R^2 values. The residuals are still clustered but it doesn't appear as drastic and the Q-Q plot is better. However, all points have a relatively high Cook's distance which is surprising and not ideal. 

## Transforming the Response Variable
```{r}
# Taking the square root of skewness
lm_skew_tr_resp1 <- lm(sqrt(skewness) ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_skew_tr_resp1)
par(mfrow = c(2,2))
plot(lm_skew_tr_resp1)
```
Taking the square root of skewness also results in a really high adjusted R^2 and slightly better model fit. The distribution of the residuals is still very clustered which is not ideal for model fit. The majority of the residuals have a Cook's distance above 0.04 which also indicates poor model performance. 

## Ridge Regression
```{r}
set.seed(325)

# Original ridge regression model

# Prepare predictors once (same rationale as above)
y <- data$skewness
X <- model.matrix(skewness ~ St + Re + logistic_Fr, data = data)[, -1]

nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```


```{r}
set.seed(325)

# Prepare predictors once (same rationale as above)
y <- data$skewness
X <- model.matrix(skewness ~ sqrt(St) + poly(Re, 2) + logistic_Fr, data = data)[, -1]


nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```

The model with the the square root transformation for St and the polynomial term for Re has a slightly lower RMSE and a much lower RMSE standard deviation. 

## Spline Model
```{r}
# Spline model with cross validation
set.seed(123)
small_data <- data |> 
  select(St, Re, logistic_Fr, skewness)
folds <- createFolds(data$skewness, k = 5)
rmse_vec <- numeric(5)
adjr2_vec <- numeric(5)

for (i in 1:5) {
  test_idx <- folds[[i]]
  train <- small_data[-test_idx, ]
  test  <- small_data[test_idx, ]

  fit <- lm(skewness ~ bs(St, df = 5) + bs(Re, df = 4) + 
              bs(logistic_Fr, df = 4), data = train)
  preds <- predict(fit, newdata = test)

  # Compute residuals and metrics
  y_true <- test$skewness
  n <- length(y_true)
  p <- length(coef(fit)) - 1        # exclude intercept for p

  rss <- sum((y_true - preds)^2)
  tss <- sum((y_true - mean(y_true))^2)

  r2 <- 1 - rss/tss
  adj_r2 <- 1 - (1 - r2) * ((n - 1)/(n - p - 1))

  # Store fold metrics
  rmse_vec[i]  <- sqrt(mean((y_true - preds)^2))
  r2_vec[i]    <- r2
  adjr2_vec[i] <- adj_r2
}

# Cross-validated metrics
mean_rmse  <- mean(rmse_vec)
mean_r2    <- mean(r2_vec)
mean_adj_r2 <- mean(adjr2_vec)
sd_rmse <- sd(rmse_vec)

c(CV_RMSE = mean_rmse,
  CV_R2 = mean_r2,
  CV_RMSE_sd = sd_rmse)
```

The spline model has a lower mean RMSE than the ridge regression models but a higher standard deviation for the RMSE. The R^2 is also fairly high but a better measure would probably be adjusted R^2. 


## Takeaways for Skewness:
- Best adjusted R^2 values is from using square root St and categorical values for both Re and Fr --> all coefficients are significant past the 5% level. The residuals are still clustered but it doesn't appear as drastic and the Q-Q plot is better. However, all points have a relatively high Cook's distance which is surprising and not ideal. 
- sqrt(Skewness) also yields very high adjusted R^2 values but the model diagnostic tools aren't great. 
- Ridge regression has a lower R^2 value but the RMSE is not terrible and might be worth sacrificing the R^2 if it means that we are able to do a train/test split
- The spline model has a lower mean RMSE than the ridge regression models but a higher standard deviation of RMSE. The R^2 is also fairly higher for the spline model.

# Kurtosis

## Linear Regression Model
```{r}
# Linear regression model

lm_kurt <- lm(kurtosis ~ St + Re + logistic_Fr, data = data)
summary(lm_kurt)
```
```{r}
par(mfrow = c(2,2))
plot(lm_kurt)
```
Initially, the results for the linear model for kurtosis are better than those for the skew model. Although there are still clusters of points for the residuals, the appears to be less systematic patterns to the clusters. The Q-Q plot is also better and there are fewer points with extremely high leverage. However, the $R^2$ value is lower than that for the skew linear model. 

```{r}
# Linear model with the best transformation from each based on EDA
# RESULTS: performs worse than the model above
lm_kurt2 <- lm(kurtosis ~ St_sqrt + Re + logistic_Fr, data = data)

summary(lm_kurt2)
par(mfrow = c(2,2))
plot(lm_kurt2)
```
This model above doesn't have better R^2, but the coefficients are all statistically significant past the 5% level. The residuals are also less clsutered and the Q-Q plot is better with fewer high leverage points.

## Linear Model with Interaction Terms
```{r}
# Interacting Re and logistic_Fr
lm_kurt_int1 <- lm(kurtosis ~ St + Re * logistic_Fr, data = data)
summary(lm_kurt_int1)
par(mfrow = c(2,2))
plot(lm_kurt_int1)
```
The interaction term slightly improves R^2, but less of the coefficients are statistically significant. The residuals also now have a clear quadratic shape with the Q-Q plot being less ideal and there being more points with a higher cook's distance.
```{r}
# Interacting St and Re
# RESULTS: R^2 is higher than above, but several points with extremely high leverage
summary(lm_skew_int2)
par(mfrow = c(2,2))
plot(lm_kurt_int2)

# Interacting St and Fr
# RESULTS: much lower R^2 and several points with extremely high leverage
lm_kurt_int3 <- lm(kurtosis ~ Re + St * logistic_Fr, data = data)
summary(lm_kurt_int3)
par(mfrow = c(2,2))
plot(lm_kurt_int3)

# Interacting all three variables
# RESULTS: few coefficeints are statistically significant and residuals aren't great
lm_kurt_int4 <- lm(kurtosis ~ Re * St * logistic_Fr, data = data)
summary(lm_kurt_int4)
par(mfrow = c(2,2))
plot(lm_kurt_int4)
```

## Polynomial Models
```{r}
# Polynomials of degree 2 for St and Re
# RESULTS: adjusted R^2 is similar to interaction model above but residual pattern and Q-Q is better
# Still have some points with high leverage
poly_kurt1 <- lm(kurtosis ~ poly(St, 2) + poly(Re, 2) + logistic_Fr, data = data)
summary(poly_kurt1)
par(mfrow = c(2,2))
plot(poly_kurt1)

# NEED to run this through CV


# Polynomials of degree 3 for St, 2 for Re
# RESULTS: adjusted R^2 is similar to interaction model above but residual pattern and Q-Q is better
# Still have some points with high leverage
poly_kurt2 <- lm(kurtosis ~ poly(St, 3) + poly(Re, 2) + logistic_Fr, data = data)
summary(poly_kurt2)
par(mfrow = c(2,2))
plot(poly_kurt2)
```
## Transforming the Input Variables
```{r}
# Treating the Fr and Re as categorical
lm_kurt_tr_inp1 <- lm(kurtosis ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_kurt_tr_inp1)
par(mfrow = c(2,2))
plot(lm_kurt_tr_inp1)
```
The R^2 value is greatly improved and almost all coefficients are statistically significant. The residual plot also has less of a pattern, Q-Q plot is better, although all points tend to have a high cook's distance.

## Transforming the Response Variable
```{r}
# Taking the square root of skewness
lm_kurt_tr_resp1 <- lm(sqrt(kurtosis) ~ sqrt(St) + as.factor(Re) + 
                    as.factor(logistic_Fr),
                  data = data)
summary(lm_kurt_tr_resp1)
par(mfrow = c(2,2))
plot(lm_kurt_tr_resp1)
```
The R^2 value is still high and almost all coefficients are statistically significant. Additionally, the residuals are slightly parabolic but the Q-Q plot is better despite most of the residuals having high leverage.

## Ridge Regression

```{r}

set.seed(325)

# Original ridge regression model

# Prepare predictors once (same rationale as above)
y <- data$kurtosis
X <- model.matrix(kurtosis ~ St + Re + logistic_Fr, data = data)[, -1]

nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```
The RMSE for this model is extremely high as well as the standard deviation. 

```{r}
set.seed(325)

# Prepare predictors once (same rationale as above)
y <- data$kurtosis
X <- model.matrix(kurtosis ~ sqrt(St) + poly(Re, 2) + logistic_Fr, data = data)[, -1]


nested_ridge <- function(X, y, K_outer = 5, K_inner = 5, seed = 325) {
  set.seed(seed)
  folds <- createFolds(y, k = K_outer, list = TRUE)
  rmse_vec <- numeric(K_outer)
  mae_vec  <- numeric(K_outer)
  r2_vec   <- numeric(K_outer)

  for (i in seq_len(K_outer)) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(seq_len(length(y)), test_idx)

    X_tr <- X[train_idx, , drop = FALSE]; y_tr <- y[train_idx]
    X_te <- X[test_idx,  , drop = FALSE]; y_te <- y[test_idx]

    # inner CV to select lambda using only training data
    cv_inner <- cv.glmnet(X_tr, y_tr, alpha = 0, nfolds = K_inner)
    lambda_star <- cv_inner$lambda.min

    # fit on training set with chosen lambda
    fit <- glmnet(X_tr, y_tr, alpha = 0, lambda = lambda_star)

    preds <- as.numeric(predict(fit, newx = X_te, s = lambda_star))
    rmse_vec[i] <- sqrt(mean((y_te - preds)^2))
    mae_vec[i]  <- mean(abs(y_te - preds))
    r2_vec[i]   <- 1 - sum((y_te - preds)^2) / sum((y_te - mean(y_tr))^2)
  }

  list(
    RMSE_mean = mean(rmse_vec), RMSE_sd = sd(rmse_vec),
    MAE_mean  = mean(mae_vec),
    R2_mean   = mean(r2_vec)
  )
}

res <- nested_ridge(X, y, K_outer = 5, K_inner = 5)
res

```

Neither of the ridge regression models perform well -- both have high RMSE mean & SD with low R^2. 

## Spline Model
```{r}
# Spline model with cross validation
set.seed(123)
small_data <- data |> 
  select(St, Re, logistic_Fr, kurtosis)
folds <- createFolds(data$kurtosis, k = 5)
rmse_vec <- numeric(5)
r2_vec <- numeric(5)

for (i in 1:5) {
  test_idx <- folds[[i]]
  train <- small_data[-test_idx, ]
  test  <- small_data[test_idx, ]

  fit <- lm(kurtosis ~ bs(St, df = 5) + bs(Re, df = 4) + 
              bs(logistic_Fr, df = 4), data = train)
  preds <- predict(fit, newdata = test)

  # Compute residuals and metrics
  y_true <- test$kurtosis
  n <- length(y_true)
  p <- length(coef(fit)) - 1        # exclude intercept for p

  rss <- sum((y_true - preds)^2)
  tss <- sum((y_true - mean(y_true))^2)

  r2 <- 1 - rss/tss
  adj_r2 <- 1 - (1 - r2) * ((n - 1)/(n - p - 1))

  # Store fold metrics
  rmse_vec[i]  <- sqrt(mean((y_true - preds)^2))
  r2_vec[i]    <- r2
  adjr2_vec[i] <- adj_r2
}

# Cross-validated metrics
mean_rmse  <- mean(rmse_vec)
mean_r2    <- mean(r2_vec)
mean_adj_r2 <- mean(adjr2_vec)
sd_rmse <- sd(rmse_vec)

c(CV_RMSE = mean_rmse,
  CV_R2 = mean_r2,
  CV_RMSE_sd = sd_rmse)
```

The spline model performs extremely poorly with very high mean RMSE and standard deviation RMSE. 

# Takeaways for Kurtosis
- The linear model with the transformations from the EDA does not have a very high R^2, but the model diagnostics are better than for the original skew models
- Polynomial models don't improve the R^2 values as much but some of the diagnostic plots are better
- Treating Fr and Re as categorical greatly improves the R^2 of the model and almost all coefficients are statistically significant. The residual plot also has less of a pattern, Q-Q plot is better, although all points tend to have a high cook's distance.
- Transforming the response gave similar R^2 values to the categorical treatment of Fr and Re but the model diagnostics are not greatly improved.
- The ridge regression models do not perform well for kurtosis. 
- The spline models do not perform well either. 

