EDA- Abby

```{r}
train <- read.csv("data-train.csv")
```

```{r}
# convert raw moments to central moments
library(moments)
library(boot)

# raw moments from training data
m1 <- train$R_moment_1   
m2 <- train$R_moment_2
m3 <- train$R_moment_3
m4 <- train$R_moment_4

# raw moments starting at 0
raw_mat0 <- cbind(m0 = 1, m1 = m1, m2 = m2, m3 = m3, m4 = m4)

# raw to central
central_all <- t(apply(raw_mat0, 1, raw2central))

output <- data.frame(
  mean        = m1, # mean- first raw moment
  mu2_central = central_all[, 3], # sd 
  mu3_central = central_all[, 4],
  mu4_central = central_all[, 5]
)

output$sd <- sqrt(output$mu2_central) # sd= sqrt(2nd central moment)
output$skewness <- output$mu3_central / (output$sd^3) # 3rd cm / sd^3
output$kurtosis <- output$mu4_central / (output$sd^4) #4th cm/ sd^4

central_moments_combined <- cbind(train, output)
```


## Exploring Models

```{r}
data <- read.csv("data_transformed.csv")
```

### Response is first central moment: mean

- Re- treat as categorical if we want it to be for inference
- Fr- logistic
- St- use log(St) or sqrt(St) for symmetry
- Mean response- log(mean) for symmetry

```{r}
# linear model
lm_linear <- lm(log(mean) ~ Re + logistic_Fr + St_sqrt, data = data)
summary(lm_linear)
plot(lm_linear)
```
Adjusted R^2: 93% but the residuals are not randomly scattered around zero-> indicates a non-linear relationship between predictors and response
U-shape in residuals indicate underfitting model


```{r}
# trying adding polynomial terms
lm_poly <- lm(log(mean) ~ poly(Re, 2) + logistic_Fr + St_sqrt, data = data)
plot(lm_poly)

anova(lm_linear, lm_poly)
AIC(lm_linear, lm_poly)
BIC(lm_linear, lm_poly)
summary(lm_poly)$adj.r.squared
```

```{r}
library(caret)

set.seed(123) 
ctrl <- trainControl(method = "cv", number = 5)

model_poly_cv <- train(
  log(mean) ~ poly(Re, 2) + logistic_Fr + St_sqrt,
  data = data,
  method = "lm",
  trControl = ctrl
)

r2_cv <- model_poly_cv$results$Rsquared

# Compute adjusted R^2
n <- nrow(data)
p <- 4 - 1  # poly(Re,2) gives 2 terms + logistic_Fr + St_sqrt = 4 predictors total
adj_r2_cv <- 1 - (1 - r2_cv) * ((n - 1) / (n - p - 1))

adj_r2_cv
```

ANOVA test suggest p <0.05, so adding curvature does improve model - this new model has much lower AIC and BIC too 

Adding quadratic term to Re seemed to capture curvature of the model 
Adjusted R^2 is 99% --> this may suggest model overfit 


```{r}
# splines
library(splines)

lm_spline <- lm(log(mean) ~ ns(Re, df = 2) +
                              ns(logistic_Fr, df = 2) +
                              ns(St_sqrt, df = 3),
                data = data)
plot(lm_spline, which = 1)

anova(lm_linear, lm_poly)
AIC(lm_linear, lm_poly)
BIC(lm_linear, lm_poly)
summary(lm_poly)$adj.r.squared

library(boot)
set.seed(1)

lm_spline1 <- glm(log(mean) ~ ns(Re, df = 2) +
                              ns(logistic_Fr, df = 2) +
                              ns(St_sqrt, df = 3),  data = data, family = gaussian())


set.seed(1)
cv_spline <- cv.glm(data, lm_spline1, K = 10)$delta[1]
cv_spline
```
Similar trend happening with the spline model

Summary:
It appears that with log(mean) as the response variable, the regular linearity assumptions aren't met. A spline and polynomial model are much better fitted, however with an adjusted R^2 of 99%, this may actually suggest model overfit. 

### Response is second central moment: standard deviation

```{r}
# lm_mu2 <- lm(mu2_central ~ Re + logistic_Fr + St_sqrt, data = data)
# 
# plot(lm_mu2, main = "Residual Diagnostics: lm_mu2")
# 
# lm2_mu2 <- lm(mu2_central ~ Re + logistic_Fr + log_St, data = data)
# 
# plot(lm_mu2, main = "Residual Diagnostics: lm_mu2")
# 
# lm2_mu22 <- lm(mu2_central ~ Re + logistic_Fr + log_St + Re*logistic_Fr, data = data)
# 
# plot(lm_mu22, main = "Residual Diagnostics: lm_mu2")
# 
# lm2_mu3 <- lm(sqrt(mu2_central) ~ Re + logistic_Fr + log_St, data = data)
# 
# plot(lm_mu2, main = "Residual Diagnostics: lm_mu3")
```

```{r}
lm_mu2 <- lm(mu2_central ~ Re + logistic_Fr + St_sqrt, data = data)
plot(lm_mu2, main = "Residual Diagnostics")
summary(lm_mu2)$adj.r.squared

# using log(mean) as response
lm_log <- lm(log(mu2_central) ~ Re + logistic_Fr + St_sqrt, data = data)
plot(lm_log, main = "Residual Diagnostics")
summary(lm_log)$adj.r.squared

# using log_St as predictor vs St_sqrt
lm_mu22 <- lm(log(mu2_central) ~ Re + logistic_Fr + log_St, data = data)
plot(lm_mu22, main = "Residual Diagnostics")
summary(lm_mu22)$adj.r.squared

# using mean^2 as response
lm_mu2 <- lm(I(mu2_central^2) ~ Re + logistic_Fr + St_sqrt, data = data)
plot(lm_mu2, main = "Residual Diagnostics")

# using St_2 as a predictor
lm_mu3 <- lm(log(mu2_central) ~ Re + logistic_Fr + St_2, data = data)
plot(lm_mu3)
summary(lm_mu3)

# other good fits

lm.fit3 <- lm(log(mu2_central) ~ St_sqrt + factor(Re)+factor(logistic_Fr), data)
plot(lm.fit3) # residual isn't as good as St_2
summary(lm.fit3) # higher adjusted R_2 of 77%

lm.fit4 <- lm(log(mu2_central) ~ St_2+ factor(Re)+factor(logistic_Fr), data)
plot(lm.fit4) # best out of linear
summary(lm.fit4)


library(caret) 

ctrl <- trainControl(method = "cv", number = 5) 

model_lin1 <- train(log(mu2_central) ~ St_sqrt + factor(Re)+ factor(logistic_Fr), data = data, method = "lm", trControl = ctrl)
model_lin1 #1.78 RMSE

ctrl2 <- trainControl(method = "cv", number = 5) 

model_lin2 <- train(log(mu2_central) ~ log_St+log_Re+logistic_Fr, data = data, method = "lm", trControl = ctrl)
model_lin2 #2.08 RMSE

```

The log transformation of the mean make the residuals more randomly scattered about zero, and addresses some outliers from the qqplot. Using log_St or St_sqrt results in around similar residuals. 
Best adjusted R^2 from log(mean) model with 62%

Best linear model: 
- linear regression: St_sqrt, factor Re, factor logistic

### New Ridge/ Lasso based on best linear

```{r}
# performing with ridge regression

ctrl <- trainControl(method = "cv", number = 5)

ridge_fit <- train(
  log(mu2_central) ~ St_sqrt + factor(Re) + factor(logistic_Fr),
  data = data,
  method = "glmnet",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(alpha = 0,        # alpha = 0 => ridge
                         lambda = 10^seq(3, -4, length.out = 100)),
  na.action = na.omit
)

# Best CV RMSE (averaged over the 5 folds at the best lambda)
ridge_fit$results$RMSE[which.min(ridge_fit$results$RMSE)]
# or simply:
min(ridge_fit$results$RMSE)
# RMSE from ridge fit- 1.78

```

```{r}
# lasso regression
ctrl <- trainControl(method = "cv", number = 5)

lasso_fit <- train(
  log(mu2_central) ~ St_sqrt + factor(Re) + factor(logistic_Fr),
  data = data,
  method = "glmnet",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(alpha = 1,         # alpha = 1 â†’ lasso
                         lambda = 10^seq(3, -4, length.out = 100)),
  na.action = na.omit
)

# Best 5-fold CV RMSE
min(lasso_fit$results$RMSE)

```


### Old Ridge/Lasso 
```{r}
# ridge regression - good for interpretability
library(glmnet)

X <- model.matrix(mu2_central ~ Re + logistic_Fr + St_sqrt, data = data)[, -1]
y <- data$mu2_central
ridge_cv <- cv.glmnet(X, y, alpha = 0)   # cross-validation to choose lambda
# plot(ridge_cv)

ridge_best_lambda <- ridge_cv$lambda.min
ridge_best_lambda

ridge_model <- glmnet(X, y, alpha = 0, lambda = ridge_best_lambda)
coef(ridge_model)

```

```{r}
# lasso regression- good for prediction

lasso_cv <- cv.glmnet(X, y, alpha = 1)
# plot(lasso_cv)

lasso_best_lambda <- lasso_cv$lambda.min
lasso_best_lambda

lasso_model <- glmnet(X, y, alpha = 1, lambda = lasso_best_lambda)
coef(lasso_model)

```

```{r}
# compare lasso and ridge
ridge_pred <- predict(ridge_model, s = ridge_best_lambda, newx = X)
lasso_pred <- predict(lasso_model, s = lasso_best_lambda, newx = X)

ridge_rmse <- sqrt(mean((ridge_pred - y)^2))
lasso_rmse <- sqrt(mean((lasso_pred - y)^2))

ridge_rmse
lasso_rmse

data.frame(
  Model = c("Ridge", "Lasso"),
  Best_Lambda = c(ridge_best_lambda, lasso_best_lambda),
  RMSE = c(ridge_rmse, lasso_rmse)
)
```

Lasso has slightly better RMSE fit- 227. Given sqrt(St), logistic Fr, and Re and a non-transformed response. 

### Spline model - cubic spline

Choosing a cubic spline b/c it avoids large oscillations at the boundaries. 
Using df = 2 or 1 knot for Re and logistic_Fr

```{r}
library(splines)

spline_model <- lm(mu2_central ~ 
                     ns(Re, df = 2) +       
                     ns(logistic_Fr, df = 2) + 
                     ns(St_sqrt, df = 3),       
                   data = data)

summary(spline_model)

summary(spline_model)$adj.r.squared
plot(spline_model)
```
Adjusted R^2 of 42%. 
Residuals don't seem to have improved by much. 

```{r}
# using log transform of SD
spline_model2 <- lm(log(mu2_central) ~ 
                     ns(Re, df = 2) +       
                     ns(logistic_Fr, df = 2) + 
                     ns(St_sqrt, df = 3),       
                   data = data)

summary(spline_model2)

summary(spline_model2)$adj.r.squared
plot(spline_model2)
```
Adjusted R^2 of 82%
Much better residuals

```{r}
# spline model cv

set.seed(42)

ctrl <- trainControl(method = "cv", number = 5)

cv_spline <- train(
  log(mu2_central) ~ 
    ns(Re, df = 2) + 
    ns(logistic_Fr, df = 2) + 
    ns(St_sqrt, df = 3),
  data = data,
  method = "lm",
  trControl = ctrl
)

# Best 5-fold CV RMSE
min(cv_spline$results$RMSE) # 1.64
```


```{r}
# using log transform of SD, log_St
spline_model3 <- lm(log(mu2_central) ~ 
                     ns(Re, df = 2) +       
                     ns(logistic_Fr, df = 2) + 
                     ns(log_St, df = 3),       
                   data = data)

summary(spline_model3)

summary(spline_model3)$adj.r.squared
plot(spline_model3)
```
Adjusted R^2 is 83%

```{r}
# calculating CV error with these two spline models
spline_model2 <- glm(log(mu2_central) ~ 
                       ns(Re, df = 2) + 
                       ns(logistic_Fr, df = 2) + 
                       ns(St_sqrt, df = 3),
                     data = data, family = gaussian())

spline_model3 <- glm(log(mu2_central) ~ 
                       ns(Re, df = 2) + 
                       ns(logistic_Fr, df = 2) + 
                       ns(log_St, df = 3),
                     data = data, family = gaussian())

spline_model1 <- glm(mu2_central ~ 
                     ns(Re, df = 2) +       
                     ns(logistic_Fr, df = 2) + 
                     ns(St_sqrt, df = 3),       
                   data = data, family = gaussian())


set.seed(1)
cv_spline2 <- cv.glm(data, spline_model2, K = 10)$delta[1]
cv_spline3 <- cv.glm(data, spline_model3, K = 10)$delta[1]
cv_spline1 <- cv.glm(data, spline_model1, K = 10)$delta[1]

cv_table <- data.frame(
  Model = c("Spline with St_sqrt", "Spline with log_St", "Spline no transform"),
  CV_MSE = c(cv_spline2, cv_spline3, cv_spline1),
  CV_RMSE = sqrt(c(cv_spline2, cv_spline3, cv_spline1))
)
print(cv_table)
```
CV MSE and RMSE is slightly higher with St_sqrt- prefer St_sqrt as predictor

Takeaway:
Out of spline models, one with log(mean) response and St_sqrt, numeric Re, and logistic Fr predictors is best model with highest adjusted R^2 and lowest CV MSE. 


### Polynomial Regression Model



