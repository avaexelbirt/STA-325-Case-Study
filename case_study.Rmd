---
title: "STA 325 Case Study"
authors: "Grady P., Laura C., Abby L., Ava E., & Ella T."
output: html_document
---

# Loading the packages and data

```{r}
## Loading the libraries 
library(tidyverse)
library(dplyr)
library(ggplot2)
# install.packages(moments)
library(moments)
library(glmnet)  
library(splines) 
library(caret)

## Loading the data
data_csv <- read.csv("Data/data-train.csv")
```

# Convert to central moments

```{r}
# convert raw moments to central moments
library(moments)

# raw moments from training data
m1 <- data_csv$R_moment_1   
m2 <- data_csv$R_moment_2
m3 <- data_csv$R_moment_3
m4 <- data_csv$R_moment_4

# raw moments starting at 0
raw_mat0 <- cbind(m0 = 1, m1 = m1, m2 = m2, m3 = m3, m4 = m4)

# raw to central
central_all <- t(apply(raw_mat0, 1, raw2central))



output <- data.frame(
  mean        = m1, # mean- first raw moment
  mu2_central = central_all[, 3], # sd 
  mu3_central = central_all[, 4], # skew
  mu4_central = central_all[, 5] # kurtosis
)

# Converting the central moments to statistics by dividing by sd
output$sd <- sqrt(output$mu2_central) # sd= sqrt(2nd central moment)
output$skewness <- output$mu3_central / (output$sd^3) # 3rd cm / sd^3
output$kurtosis <- output$mu4_central / (output$sd^4) #4th cm/ sd^4

central_moments_combined <- cbind(data_csv, output)
```

# Transform Predictors

```{r}
# Final dataframe with transformed predictors as well
data <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logitistic transformation for inf
         )
```

# Distribution of St, Re, Fr

### St: size of particles

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = St)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
  labs(title = "Histogram of St", x = "St", y = "Count")

# log transformed
ggplot(data, aes(x = log_St)) +
  geom_histogram(bins = 30, fill = "purple", color = "black") +
  labs(title = "Histogram of log(St)", x = "log(St)", y = "Count")

# squared transformed
ggplot(data, aes(x = St_2)) +
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  labs(title = "Histogram of St^2", x = "St^2", y = "Count")

# square root transformed
ggplot(data, aes(x = St_sqrt)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(St)", x = "sqrt(St)", y = "Count")

```

Square root and log transformations have the most symmetric distributions. Square root is preferred since the range of St values is positive, so square root transformation will preserve the sign.

### Re: intensity of the turbulence within the flow

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = Re)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Re", x = "Re", y = "Count")

# log tranformed
ggplot(data, aes(x = log_Re)) +
  geom_histogram(bins = 30, fill = "purple", color = "black") +
  labs(title = "Histogram of log(Re)", x = "log(Re)", y = "Count")

# squared transformed
ggplot(data, aes(x = Re_2)) +
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  labs(title = "Histogram of Re^2", x = "Re^2", y = "Count")

# square root transformed
ggplot(data, aes(x = Re_sqrt)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Re)", x = "sqrt(Re)", y = "Count")

# Finding the unique values of Re
unique(data$Re)
table(data$Re)
```

Notice Re only has 3 possible values: 90, 224, 398. This likely means that there are three main types of clouds so it might be advantageous to try use a categorical variable at points for interpretability. The distributions across all transformations are fairly similar since there are only three distinct Re values.

### Fr: degree of gravitation acceleration experienced by the flow

Note: This variable has infinite values

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = Fr)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Fr", x = "Fr", y = "Count")

# logistic transformation
ggplot(data, aes(x = logistic_Fr)) +
  geom_histogram(bins = 30, fill = "green", color = "black") +
  labs(title = "Histogram of logistic of Fr", x = "Transformed Fr", y = "Count")

# Finding the unique values of Re
table(data$Fr)


```

Similarly to Re above, Fr only has three possible values: 0.052, 0.300, and Inf. The distribution of these variables is fairly even with 0.052 being the most common value and 0.3 being the least common value. We decided to use the logit transformation for Fr so that the values are bound between 0 and 1 which will handle the problem with the infinite values.

## Key Takeaways

-   St: log(St) and sqrt(St) both have more symmetric distributions - prefer sqrt(St) since all transformed values are still positive

-   Re: Could be treated categorically since there are only 3 unique cloud types. None of the transformations greatly changed the distribution of the variable since there are only 3 unique types so continue investigating the impact of different transformations in the models.

-   Fr: Use logistic transformation of the variable to ensure all values are bound between 0 and 1 to address the infinite values. Also explore treating Fr as categorical since there are 3 unique values.

# EDA for Summary Statistics

### Mean: first central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = mean)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Mean", x = "Mean", y = "Count")

# log transformed
ggplot(data, aes(x = log(mean))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Mean)", x = "log(Mean)", y = "Count")

# squared transformed
ggplot(data, aes(x = mean^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Mean^2", x = "Mean^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(mean))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Mean)", x = "sqrt(Mean)", y = "Count")

```

Mean is very right-skewed with the majority of the means concentrated around 0. log(Mean) does make the distribution more symmetric, but converts all of the values to negative which is less interpretable.

### Standard Deviation: second central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = sd)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count")

# log transformed
ggplot(data, aes(x = log(sd))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Standard Deviation)", x = "log(Standard Deviation)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = sd^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Standard Deviation^2", x = "Standard Deviation^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(sd))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Standard Deviation)", x = "sqrt(Standard Deviation)", 
       y = "Count")

```

We notice a similar pattern in that Standard Deviation is also very right skewed with the majority of the values close to 0. Log transforming Standard Deviation does make the distribution more symmetric, but also negative which is less interpretable.

### Skewness: third central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = skewness)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Skew", x = "Skew", y = "Count")

# log transformed
ggplot(data, aes(x = log(skewness))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Skew)", x = "log(Skew)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = skewness^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Skew^2", x = "Skew^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(skewness))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Skew)", x = "sqrt(Skew)", 
       y = "Count")
```

Skew has a bi-modal distribution with there being very few observations with a skew between 100 and 200. None of the transformations appear to improve the distribution of skew, often making the distribution tri-modal with clusters instead of more symmetric. For this reason, it might be best to stick with the untransformed values for skew.

The three distinct distributions from the transformation could relate to the potentially categorical nature of Fr and Re.

### Kurtosis: fourth central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = kurtosis)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Kurtosis", x = "Kurtosis", y = "Count")

# log transformed
ggplot(data, aes(x = log(kurtosis))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Kurtosis)", x = "log(Kurtosis)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = kurtosis^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Kurtosis^2", x = "Kurtosis^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(kurtosis))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Kurtosis)", x = "sqrt(Kurtosis)", 
       y = "Count")
```

Kurtosis is also right-skewed but not as drastically as mean and standard deviation above. Compared to the transformed distributions for Kurtosis, the untransformed distribution is more symmetric which would indicate that keeping kurtosis untransformed might yield the best results.

The three distinct distributions from the transformation could relate to the potentially categorical nature of Fr and Re.

## Key Takeaways

-   Mean: Very right skewed with the majority of the values around 0 - log(Mean) has the most symmetric distribution but has negative values for mean which are less interpretable given the untransformed means are all positive.

-   Standard Deviation: Very right skewed (similar distribution to mean) – log(Sd) is the most symmetrical of the distributions but similar concerns regarding interpreting a negative standard deviation.

-   Skew: Bimodal (almost trimodal) with the distributions centered around 90 and 260. The transformations created three distinct distributions which could be caused by the categorical nature of both the Fr and Re variables (maybe the distinct skew distributions are connected to each cloud type).

-   Kurtosis: Right skewed and bimodal – log(kurtosis) has three distinct distributions which could connect to the categorical nature of Fr and Re with specific cloud types.






## Models for Mean 
First we will fit 5 different models to predict mean. The first is a simple linear model, second is a linear model with interaction terms, polynomial model with quadratic terms, Ridge Regression, and a Spline Model. 
```{r}
set.seed(325)

## Model 1: Simple Linear Model
model_mean_linear <- lm(mean ~ sqrt(St) + Re + logistic_Fr, data = data)
summary(model_mean_linear)



## Model 2: Linear Model with Interactions
model_mean_interact <- lm(mean ~ sqrt(St) * Re * logistic_Fr, data = data)
summary(model_mean_interact)



## Model 3: Polynomial Model
model_mean_poly <- lm(mean ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
                        sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, 
                      data = data)
summary(model_mean_poly)



## Model 4: Ridge Regression
X_mean <- model.matrix(mean ~ sqrt(St) * Re * logistic_Fr + 
                         I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), data = data)[,-1]
y_mean <- data$mean

cv_ridge_mean <- cv.glmnet(X_mean, y_mean, alpha = 0, nfolds = 5)
model_mean_ridge <- glmnet(X_mean, y_mean, alpha = 0, lambda = cv_ridge_mean$lambda.min)




## Model 5: Spline Model
model_mean_spline <- lm(mean ~ bs(sqrt(St), df = 4) + bs(Re, df = 3) + 
                          bs(logistic_Fr, df = 3) +
                          sqrt(St):Re + sqrt(St):logistic_Fr, 
                        data = data)
summary(model_mean_spline)
```

Next, we will use 5-fold CV for all 5 mean models to see which is the best overall performing model.
```{r}
set.seed(325)
k_folds <- 5
folds <- createFolds(data$mean, k = k_folds, list = TRUE)

cv_results_mean <- data.frame(
  Model = c("Linear", "Interaction", "Polynomial", "Ridge", "Spline"),
  RMSE = rep(0, 5),
  MAE = rep(0, 5),
  R_squared = rep(0, 5)
)

calc_metrics <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)^2))
  mae <- mean(abs(actual - predicted))
  r2 <- 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
  return(c(rmse, mae, r2))
}

for (model_idx in 1:5) {
  fold_rmse <- numeric(k_folds)
  fold_mae <- numeric(k_folds)
  fold_r2 <- numeric(k_folds)
  
  for (i in 1:k_folds) {
    test_indices <- folds[[i]]
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    if (model_idx == 1) {
      #linear
      fit <- lm(mean ~ sqrt(St) + Re + logistic_Fr, data = train_data)
      predictions <- predict(fit, newdata = test_data)
      
    } else if (model_idx == 2) {
      #interact
      fit <- lm(mean ~ sqrt(St) * Re * logistic_Fr, data = train_data)
      predictions <- predict(fit, newdata = test_data)
      
    } else if (model_idx == 3) {
      #poly
      fit <- lm(mean ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
                  sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, 
                data = train_data)
      predictions <- predict(fit, newdata = test_data)
    } else if (model_idx == 4) {
      #ridge
      X_train <- model.matrix(mean ~ sqrt(St) * Re * logistic_Fr + 
                                I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), 
                              data = train_data)[,-1]
      y_train <- train_data$mean
      X_test <- model.matrix(mean ~ sqrt(St) * Re * logistic_Fr + 
                               I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), 
                             data = test_data)[,-1]
      
      cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 5)
      fit <- glmnet(X_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)
      predictions <- predict(fit, newx = X_test, s = cv_ridge$lambda.min)
    } else if (model_idx == 5) {
      #spline
      fit <- lm(mean ~ bs(sqrt(St), df = 4) + bs(Re, df = 3) + 
                  bs(logistic_Fr, df = 3) +
                  sqrt(St):Re + sqrt(St):logistic_Fr, 
                data = train_data)
      predictions <- predict(fit, newdata = test_data)
    }
    

    metrics <- calc_metrics(test_data$mean, predictions)
    fold_rmse[i] <- metrics[1]
    fold_mae[i] <- metrics[2]
    fold_r2[i] <- metrics[3]
  }

  cv_results_mean$RMSE[model_idx] <- mean(fold_rmse)
  cv_results_mean$MAE[model_idx] <- mean(fold_mae)
  cv_results_mean$R_squared[model_idx] <- mean(fold_r2)
}

#show results
print("=== CROSS-VALIDATION RESULTS FOR MEAN MODELS ===")
print(cv_results_mean)
print(paste("Best model for Mean:", cv_results_mean$Model[which.min(cv_results_mean$RMSE)]))

ggplot(cv_results_mean, aes(x = reorder(Model, RMSE), y = RMSE)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(RMSE, 4)), vjust = -0.5) +
  labs(title = "5-Fold CV RMSE for Mean Prediction Models",
       x = "Model Type", y = "RMSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


## Models for Kurtosis 
First we will fit 5 different models to predict kurtosis The first is a simple linear model, second is a linear model with interaction terms, polynomial model with quadratic terms, Ridge Regression, and a Spline Model. 
```{r}
set.seed(325)
 
# Model 1: Simple Linear Model
model_kurt_linear <- lm(kurtosis ~ sqrt(St) + Re + logistic_Fr, data = data)
summary(model_kurt_linear)

# Model 2: Linear Model with Interactions
model_kurt_interact <- lm(kurtosis ~ sqrt(St) * Re * logistic_Fr, data = data)
summary(model_kurt_interact)

# Model 3: Polynomial Model
model_kurt_poly <- lm(kurtosis ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
                        sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, 
                      data = data)
summary(model_kurt_poly)

# Model 4: Ridge Regression
X_kurt <- model.matrix(kurtosis ~ sqrt(St) * Re * logistic_Fr + 
                         I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), data = data)[,-1]
y_kurt <- data$kurtosis

cv_ridge_kurt <- cv.glmnet(X_kurt, y_kurt, alpha = 0, nfolds = 5)
model_kurt_ridge <- glmnet(X_kurt, y_kurt, alpha = 0, lambda = cv_ridge_kurt$lambda.min)

# Model 5: Spline Model
model_kurt_spline <- lm(kurtosis ~ bs(sqrt(St), df = 4) + bs(Re, df = 3) + 
                          bs(logistic_Fr, df = 3) +
                          sqrt(St):Re + sqrt(St):logistic_Fr, 
                        data = data)
summary(model_kurt_spline)

```


Next, we will use 5-fold CV for all 5 mean models to see which is the best overall performing model.

```{r}
set.seed(325)
cv_results_kurt <- data.frame(
  Model = c("Linear", "Interaction", "Polynomial", "Ridge", "Spline"),
  RMSE = rep(0, 5),
  MAE = rep(0, 5),
  R_squared = rep(0, 5)
)

for (model_idx in 1:5) {
  fold_rmse <- numeric(k_folds)
  fold_mae <- numeric(k_folds)
  fold_r2 <- numeric(k_folds)
  
  for (i in 1:k_folds) {
    test_indices <- folds[[i]]
    train_data <- data[-test_indices, ]
    test_data <- data[test_indices, ]
    
    if (model_idx == 1) {
      #linear
      fit <- lm(kurtosis ~ sqrt(St) + Re + logistic_Fr, data = train_data)
      predictions <- predict(fit, newdata = test_data)
      
    } else if (model_idx == 2) {
      #interact
      fit <- lm(kurtosis ~ sqrt(St) * Re * logistic_Fr, data = train_data)
      predictions <- predict(fit, newdata = test_data)
      
    } else if (model_idx == 3) {
      #poly
      fit <- lm(kurtosis ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
                  sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, 
                data = train_data)
      predictions <- predict(fit, newdata = test_data)
      
    } else if (model_idx == 4) {
      #ridge
      X_train <- model.matrix(kurtosis ~ sqrt(St) * Re * logistic_Fr + 
                                I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), 
                              data = train_data)[,-1]
      y_train <- train_data$kurtosis
      X_test <- model.matrix(kurtosis ~ sqrt(St) * Re * logistic_Fr + 
                               I(sqrt(St)^2) + I(Re^2) + I(logistic_Fr^2), 
                             data = test_data)[,-1]
      
      cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 5)
      fit <- glmnet(X_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)
      predictions <- predict(fit, newx = X_test, s = cv_ridge$lambda.min)
      
    } else if (model_idx == 5) {
      #spline
      fit <- lm(kurtosis ~ bs(sqrt(St), df = 4) + bs(Re, df = 3) + 
                  bs(logistic_Fr, df = 3) +
                  sqrt(St):Re + sqrt(St):logistic_Fr, 
                data = train_data)
      predictions <- predict(fit, newdata = test_data)
    }
    
    metrics <- calc_metrics(test_data$kurtosis, predictions)
    fold_rmse[i] <- metrics[1]
    fold_mae[i] <- metrics[2]
    fold_r2[i] <- metrics[3]
  }
  
  cv_results_kurt$RMSE[model_idx] <- mean(fold_rmse)
  cv_results_kurt$MAE[model_idx] <- mean(fold_mae)
  cv_results_kurt$R_squared[model_idx] <- mean(fold_r2)
}

#results
print("=== CROSS-VALIDATION RESULTS FOR KURTOSIS MODELS ===")
print(cv_results_kurt)
print(paste("Best model for Kurtosis:", cv_results_kurt$Model[which.min(cv_results_kurt$RMSE)]))

ggplot(cv_results_kurt, aes(x = reorder(Model, RMSE), y = RMSE)) +
  geom_bar(stat = "identity", fill = "coral") +
  geom_text(aes(label = round(RMSE, 4)), vjust = -0.5) +
  labs(title = "5-Fold CV RMSE for Kurtosis Prediction Models",
       x = "Model Type", y = "RMSE") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

