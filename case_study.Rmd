---
title: "STA 325 Case Study"
authors: "Grady P., Laura C., Abby L., Ava E., & Ella T."
output: html_document
---

# Loading the packages and data

```{r}
## Loading the libraries 
library(tidyverse)
library(dplyr)
library(ggplot2)
# install.packages(moments)
library(moments)
library(glmnet)  
library(splines) 
library(caret)
library(boot)
library(olsrr)
library(MASS)

## Loading the data
data_csv <- read.csv("Data/data-train.csv")
data_test <- read.csv("Data/data-test.csv")
```

# Convert to central moments

```{r}
# convert raw moments to central moments
library(moments)

# raw moments from training data
m1 <- data_csv$R_moment_1   
m2 <- data_csv$R_moment_2
m3 <- data_csv$R_moment_3
m4 <- data_csv$R_moment_4

# raw moments starting at 0
raw_mat0 <- cbind(m0 = 1, m1 = m1, m2 = m2, m3 = m3, m4 = m4)

# raw to central
central_all <- t(apply(raw_mat0, 1, raw2central))



output <- data.frame(
  mean        = m1, # mean- first raw moment
  mu2_central = central_all[, 3], # sd 
  mu3_central = central_all[, 4], # skew
  mu4_central = central_all[, 5] # kurtosis
)

# Converting the central moments to statistics by dividing by sd
output$sd <- sqrt(output$mu2_central) # sd= sqrt(2nd central moment)
output$skewness <- output$mu3_central / (output$sd^3) # 3rd cm / sd^3
output$kurtosis <- output$mu4_central / (output$sd^4) #4th cm/ sd^4

central_moments_combined <- cbind(data_csv, output)
```

# Transform Predictors

```{r}
# Final dataframe with transformed predictors as well
data <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logitistic transformation for inf
         )

data_test <- central_moments_combined |>
  mutate(log_St = log(St), 
         log_Re = log(Re),
         St_2 = St**2,
         Re_2 = Re**2,
         Fr_2 = Fr**2,
         St_sqrt = sqrt(St),
         Re_sqrt = sqrt(Re),
         logistic_Fr = 1/(1+exp(-Fr)) # using logitistic transformation for inf
         )
```

# Distribution of St, Re, Fr
```{r}
data_long <- data %>%
  dplyr::select(Fr, Re, St) %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Value") %>%
  mutate(Transformation = "Original") %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), sqrt)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Sqrt")
  ) %>%
  bind_rows(
    data %>%
      dplyr::select(Fr, Re, St) %>%
      mutate(across(everything(), log)) %>%
      pivot_longer(cols = everything(),
                   names_to = "Variable", 
                   values_to = "Value") %>%
      mutate(Transformation = "Log")
  )
ggplot(data_long, aes(x = Value, fill = Transformation)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  facet_grid(Variable ~ Transformation, scales = "free") +
  scale_fill_manual(values = c("Original" = "skyblue", 
                               "Sqrt" = "lightgreen", 
                               "Log" = "salmon")) +
  labs(title = "Histograms of Fr, Re, St (Original, Sqrt, Log)",
       x = "Value", y = "Count") +
  theme_minimal()

```


```{r}

library(corrplot)

# Select relevant variables from your dataset
vars <- data[, c("Re", "St_sqrt", "logistic_Fr", 
                 "R_moment_1", "R_moment_2", "R_moment_3", "R_moment_4")]

# Compute correlation matrix
corr_matrix <- cor(vars, use = "complete.obs")  # handles missing values

# Plot correlation matrix
corrplot(corr_matrix, method = "color", 
         type = "upper", 
         addCoef.col = "black",  # show correlation coefficients
         tl.col = "black",       # axis labels color
         tl.srt = 45)            # label rotation
```

### St: size of particles

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = St)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
  labs(title = "Histogram of St", x = "St", y = "Count")

# log transformed
ggplot(data, aes(x = log_St)) +
  geom_histogram(bins = 30, fill = "purple", color = "black") +
  labs(title = "Histogram of log(St)", x = "log(St)", y = "Count")

# squared transformed
ggplot(data, aes(x = St_2)) +
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  labs(title = "Histogram of St^2", x = "St^2", y = "Count")

# square root transformed
ggplot(data, aes(x = St_sqrt)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(St)", x = "sqrt(St)", y = "Count")

```

Square root and log transformations have the most symmetric distributions. Square root is preferred since the range of St values is positive, so square root transformation will preserve the sign.

### Re: intensity of the turbulence within the flow

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = Re)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Re", x = "Re", y = "Count")

# log tranformed
ggplot(data, aes(x = log_Re)) +
  geom_histogram(bins = 30, fill = "purple", color = "black") +
  labs(title = "Histogram of log(Re)", x = "log(Re)", y = "Count")

# squared transformed
ggplot(data, aes(x = Re_2)) +
  geom_histogram(bins = 30, fill = "pink", color = "black") +
  labs(title = "Histogram of Re^2", x = "Re^2", y = "Count")

# square root transformed
ggplot(data, aes(x = Re_sqrt)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Re)", x = "sqrt(Re)", y = "Count")

# Finding the unique values of Re
unique(data$Re)
table(data$Re)
```

Notice Re only has 3 possible values: 90, 224, 398. This likely means that there are three main types of clouds so it might be advantageous to try use a categorical variable at points for interpretability. The distributions across all transformations are fairly similar since there are only three distinct Re values.

### Fr: degree of gravitation acceleration experienced by the flow

Note: This variable has infinite values

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = Fr)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Fr", x = "Fr", y = "Count")

# logistic transformation
ggplot(data, aes(x = logistic_Fr)) +
  geom_histogram(bins = 30, fill = "green", color = "black") +
  labs(title = "Histogram of logistic of Fr", x = "Transformed Fr", y = "Count")

# Finding the unique values of Re
table(data$Fr)


```

Similarly to Re above, Fr only has three possible values: 0.052, 0.300, and Inf. The distribution of these variables is fairly even with 0.052 being the most common value and 0.3 being the least common value. We decided to use the logit transformation for Fr so that the values are bound between 0 and 1 which will handle the problem with the infinite values.

## Key Takeaways

-   St: log(St) and sqrt(St) both have more symmetric distributions - prefer sqrt(St) since all transformed values are still positive

-   Re: Could be treated categorically since there are only 3 unique cloud types. None of the transformations greatly changed the distribution of the variable since there are only 3 unique types so continue investigating the impact of different transformations in the models.

-   Fr: Use logistic transformation of the variable to ensure all values are bound between 0 and 1 to address the infinite values. Also explore treating Fr as categorical since there are 3 unique values.

# EDA for Summary Statistics

### Mean: first central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = mean)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Mean", x = "Mean", y = "Count")

# log transformed
ggplot(data, aes(x = log(mean))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Mean)", x = "log(Mean)", y = "Count")

# squared transformed
ggplot(data, aes(x = mean^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Mean^2", x = "Mean^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(mean))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Mean)", x = "sqrt(Mean)", y = "Count")

```

Mean is very right-skewed with the majority of the means concentrated around 0. log(Mean) does make the distribution more symmetric, but converts all of the values to negative which is less interpretable.

### Standard Deviation: second central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = sd)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count")

# log transformed
ggplot(data, aes(x = log(sd))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Standard Deviation)", x = "log(Standard Deviation)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = sd^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Standard Deviation^2", x = "Standard Deviation^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(sd))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Standard Deviation)", x = "sqrt(Standard Deviation)", 
       y = "Count")

```

We notice a similar pattern in that Standard Deviation is also very right skewed with the majority of the values close to 0. Log transforming Standard Deviation does make the distribution more symmetric, but also negative which is less interpretable.

### Skewness: third central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = skewness)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Skew", x = "Skew", y = "Count")

# log transformed
ggplot(data, aes(x = log(skewness))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Skew)", x = "log(Skew)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = skewness^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Skew^2", x = "Skew^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(skewness))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Skew)", x = "sqrt(Skew)", 
       y = "Count")
```

Skew has a bi-modal distribution with there being very few observations with a skew between 100 and 200. None of the transformations appear to improve the distribution of skew, often making the distribution tri-modal with clusters instead of more symmetric. For this reason, it might be best to stick with the untransformed values for skew.

The three distinct distributions from the transformation could relate to the potentially categorical nature of Fr and Re.

### Kurtosis: fourth central moment

```{r}
# Histogram for the different input variables

# no transformations
ggplot(data, aes(x = kurtosis)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(title = "Histogram of Kurtosis", x = "Kurtosis", y = "Count")

# log transformed
ggplot(data, aes(x = log(kurtosis))) +
  geom_histogram(fill = "purple", color = "black") +
  labs(title = "Histogram of log(Kurtosis)", x = "log(Kurtosis)", 
       y = "Count")

# squared transformed
ggplot(data, aes(x = kurtosis^2)) +
  geom_histogram(fill = "pink", color = "black") +
  labs(title = "Histogram of Kurtosis^2", x = "Kurtosis^2", y = "Count")

# square root transformed
ggplot(data, aes(x = sqrt(kurtosis))) +
  geom_histogram(fill = "blue", color = "black") +
  labs(title = "Histogram of sqrt(Kurtosis)", x = "sqrt(Kurtosis)", 
       y = "Count")
```

Kurtosis is also right-skewed but not as drastically as mean and standard deviation above. Compared to the transformed distributions for Kurtosis, the untransformed distribution is more symmetric which would indicate that keeping kurtosis untransformed might yield the best results.

The three distinct distributions from the transformation could relate to the potentially categorical nature of Fr and Re.

## Key Takeaways

-   Mean: Very right skewed with the majority of the values around 0 - log(Mean) has the most symmetric distribution but has negative values for mean which are less interpretable given the untransformed means are all positive.

-   Standard Deviation: Very right skewed (similar distribution to mean) – log(Sd) is the most symmetrical of the distributions but similar concerns regarding interpreting a negative standard deviation.

-   Skew: Bimodal (almost trimodal) with the distributions centered around 90 and 260. The transformations created three distinct distributions which could be caused by the categorical nature of both the Fr and Re variables (maybe the distinct skew distributions are connected to each cloud type).

-   Kurtosis: Right skewed and bimodal – log(kurtosis) has three distinct distributions which could connect to the categorical nature of Fr and Re with specific cloud types.

# Modeling Mean

An ANOVA test revealed that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19) on explaining mean cluster volume. All three interaction terms were statistically significant (p < 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p < 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions. To further assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

To predict the mean particle cluster volume, we fit a polynomial regression model on the log-transformed response. The log transformation was necessary because without it, the residuals showed clear patterns and heteroscedasticity. After transformation, the residuals appear to be randomly scattered around zero with constant variance. The log transformation also addressed the right-skewed distribution of mean cluster volumes in the raw data.

```{r}
set.seed(325)

model_mean_poly <- lm(log(mean) ~ poly(sqrt(St), 2) + 
                                poly(Re, 2) + 
                                poly(logistic_Fr, 2) +
                                sqrt(St):Re + 
                                sqrt(St):logistic_Fr + 
                                Re:logistic_Fr, 
                      data = data)

summary(model_mean_poly)           
```

The final model includes second-degree polynomial terms for all three predictors (√St, Re, and the logistic-transformed Fr) along with three two-way interactions: √St:Re, √St:logistic_Fr, and Re:logistic_Fr. This model achieved great fit, with an adjusted R² of 0.9978, meaning it explains over 99% of the variation in log(mean) cluster volume. The model's RMSE of 0.098 on the log scale translates to predictions that are typically within about 10% of the true mean cluster volume when back-transformed.

```{r}
print(anova(model_mean_poly))
cat("\nAIC:", AIC(model_mean_poly), "\n")
cat("BIC:", BIC(model_mean_poly), "\n")
RMSE <- sqrt(mean(resid(model_mean_poly)^2))
cat("RMSE:", RMSE, "\n")

#cp formula
n <- nrow(data)
p <- length(coef(model_mean_poly))
sse <- sum(resid(model_mean_poly)^2)
sigma2_full <- summary(model_mean_poly)$sigma^2
Cp <- sse / sigma2_full - (n - 2 * p)
cat("Mallows' Cp:", Cp, "\n")

par(mfrow = c(1, 1))
plot(fitted(model_mean_poly), resid(model_mean_poly),
     main = "Residual Plot",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 19, col = "steelblue")
abline(h = 0, col = "red", lwd = 2)
```

The ANOVA table reveals which predictors contribute most substantially to explaining mean cluster volume. The results show that Reynolds number has by far the largest effect (Sum of Squares = 431.68), followed by Stokes number (SS = 7.53), and then Froude number (SS = 0.19). All three interaction terms were statistically significant (p \< 0.05), with the Re:logistic_Fr interaction contributing the most (SS = 0.48, p \< 0.001). The significant √St:logistic_Fr interaction (p = 0.001) and marginally significant √St:Re interaction (p = 0.030) suggest that the effects of particle inertia (St) on mean cluster size depend on both gravitational and turbulence conditions.

```{r}
set.seed(325)

train_control <- trainControl(method = "cv", number = 5)

cv_model <- train(
  log(mean) ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
    sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr,
  data = data,
  method = "lm",
  trControl = train_control
)

print(cv_model)
```

To assess whether this complex model overfits the data, we performed 5-fold cross-validation, averaging the RMSE across folds to estimate out-of-sample prediction error. The cross-validated RMSE was 0.118, only slightly higher than the training RMSE of 0.098, and the CV R² (0.9974) remained nearly identical to the training R² (0.9978). This close agreement between training and validation performance indicates that the model generalizes well to unseen data and does not appear to overfit the data despite its polynomial and interaction terms.

Original version of predict without CI

```{r}
pred_log_mean <- predict(model_mean_poly, newdata = data_test)
output <- data.frame(data_test, pred_log_mean = pred_log_mean)
write.csv(output, "Data/pred-mean.csv", row.names = FALSE)
```

Overall, from fitting the mean with a polynomial model, we see that Reynolds number shows a strong negative linear effect and positive quadratic effect, indicating that mean cluster volume initially decreases sharply with increasing turbulence intensity, then levels off at higher Re values. The Stokes number exhibits a similar but weaker pattern—cluster volumes first increase then decrease with particle inertia. The Froude number's negative linear and positive quadratic effects suggest that gravitational acceleration initially reduces clustering but this effect diminishes at extreme gravity levels. The significant Re:Fr interaction indicates that gravity's effect on clustering becomes stronger in more turbulent flows, which makes physical sense as turbulence and gravity compete in determining particle settling behavior. Overall, we found that Reynolds number (turbulence intensity) is by far the dominant factor controlling mean cluster size, but its effect depends significantly on gravitational conditions, specifically gravity's impact on clustering strengthens in more turbulent flows. This interaction means that predicting particle behavior requires considering both turbulence and gravity together, rather than treating them as independent effects.

[BELOW DOESNT RUN NEED HELP] Now we will predict the test set mean values with 95% confidence intervals and save it to a new file.

```{r}
#vary St, hold Re & logistic_Fr at means
Re_mean <- mean(data_finite$Re, na.rm = TRUE)
logFr_mean <- mean(data_finite$logistic_Fr, na.rm = TRUE)

grid_mean <- data.frame(
  St = seq(min(data_finite$St), max(data_finite$St), length.out = 200),
  Re = Re_mean,
  logistic_Fr = logFr_mean
) |>
  mutate(
    log_St = log(St),
    log_Re = log(Re),
    St_2 = St^2,
    Re_2 = Re^2,
    St_sqrt = sqrt(St),
    Re_sqrt = sqrt(Re)
  )
pred_mean_ci <- predict(model_mean_poly, newdata = grid, interval = "confidence", level = 0.95)
pred_mean_out <- cbind(grid, pred_mean_ci)

write.csv(pred1_out, "Data/pred_mean_out.csv", row.names = FALSE)


```

$$
\log(\text{mean}) = \beta_0 
+ \beta_1 \, \text{poly}_1(\sqrt{St}) 
+ \beta_2 \, \text{poly}_2(\sqrt{St}) 
+ \beta_3 \, \text{poly}_1(Re) 
+ \beta_4 \, \text{poly}_2(Re) 
+ \beta_5 \, \text{poly}_1(\text{logisticFr}) 
+ \beta_6 \, \text{poly}_2(\text{logisticFr}) 
+ \beta_7 \, (\sqrt{St} \cdot Re) 
+ \beta_8 \, (\sqrt{St} \cdot \text{logisticFr}) 
+ \beta_9 \, (Re \cdot \text{logisticFr}) 
$$

Confidence intervals 

```{r}
mean_model <- lm(log(mean) ~ poly(sqrt(St), 2) + poly(Re, 2) + poly(logistic_Fr, 2) +
    sqrt(St):Re + sqrt(St):logistic_Fr + Re:logistic_Fr, data= data)

# new df that varies Re while fixing St and Fr at their means
newdata_mean <- with(data, data.frame(
  Re          = seq(min(Re, na.rm = TRUE), max(Re, na.rm = TRUE), length.out = 100),
  St          = rep(mean(St, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for Re values
pred_meanRe <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanRe <- cbind(newdata_mean, pred_meanRe)

plot_meanRe1 <- ggplot(plot_meanRe, aes(x= Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies St while fixing Re and Fr at their means
newdata_mean <- with(data, data.frame(
  St          = seq(min(St, na.rm = TRUE), max(St, na.rm = TRUE), length.out = 100),
  Re          = rep(mean(Re, na.rm = TRUE), 100),
  logistic_Fr = rep(mean(logistic_Fr, na.rm = TRUE), 100)
))

# 95% CI for St values
pred_meanSt <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanSt <- cbind(newdata_mean, pred_meanSt)

plot_meanSt1 <- ggplot(plot_meanSt, aes(x= St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "St",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

# new df that varies Fr while fixing Re and St at their means
newdata_mean <- with(data, data.frame(
  logistic_Fr = seq(min(data$logistic_Fr, na.rm = TRUE), max(data$logistic_Fr, na.rm = TRUE), length.out = 100),
  Re  = rep(mean(Re, na.rm = TRUE), 100),
  St = rep(mean(St, na.rm = TRUE), 100)
))

# 95% CI for Fr values
pred_meanFr <- predict(mean_model, newdata = newdata_mean,
                       interval = "confidence", level = 0.95)

plot_meanFr <- cbind(newdata_mean, pred_meanFr)

plot_meanFr1 <- ggplot(plot_meanFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Fr",
    y = "Predicted log(mean)"
  ) +
  theme_minimal()

(plot_meanRe1 | plot_meanSt1 | plot_meanFr1)

```

The plots depict the 95% CI for the predicted log(mean) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are very confident that there appears to be a strong negative relationship between Re and log(mean). For St, we are fairly confident that there is a nonlinear and positive relationship between St and log(mean). However, the wide confidence intervals at the edges depict some areas of uncertainty. For Fr, we have some confidence that the relationship between logistic(Fr) and log(mean) is U-shaped, such that at low and high values, log(mean)is higher and values in the middle are lowest. There appears to be some uncertainty near the curvature, but the pattern appears to be overall evident of a non-monotonic effect of logistic_Fr on log(mean). 

# Standard Deviation
$$
Fr_{logistic} = \frac{1}{1+exp(-Fr)} \\
log(\hat{sd}) = 26.5 + 0.642 log(St) - 4.915 log(Re) -20.704 Fr_{logistic} -0.027log(St)log(Re) - 0.102log(St)Fr_{logistic}+3.557log(Re)Fr_{logistic}
$$
```{r}
{r}
# EDA for standard deviation
p1 <- ggplot(data, aes(x = sd)) +
      geom_histogram(fill = "skyblue", color = "black") +
      labs(title = "Histogram of Standard Deviation", x = "Standard Deviation", y = "Count") + 
  theme_minimal()

bc <- boxcox(lm(sd ~ 1, data = data), plotit = FALSE)
bc_df <- data.frame(lambda = bc$x, logLik = bc$y)

p2 <- ggplot(bc_df, aes(x = lambda, y = logLik)) +
  geom_line(color = "black") +
  geom_vline(xintercept = bc$x[which.max(bc$y)], color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype="dashed") + 
  labs(title = "Box-Cox Transformation", x = "Lambda", y = "Log-Likelihood") +
  theme_minimal()

(p1 | p2)
```

We were given a training and testing dataset, so I trained different linear (with and without ridge and lasso), polynomial and spline regression models comparing the adjusted R-squared the p-values, and used 5-fold CV to compare the top model to determine which model had the best prediction accuracy while working on new, unseen data.

After multiple models were compared from their adjusted R-squared and 5-fold CV, the linear model with all of the interaction terms was selected based on the balance between predictive accuracy (RMSE = 2.01, Adjusted R-squared = 0.74). The interaction terms were included to capture the potential effects of predictors coupled together. The linear model is interpretable and less risky to overfit, which is helpful in our goal to predict the standard deviations of the distributions given St, Re, and Fr.

```{r}
{r}
model_summary <- data.frame(
  Model = c(
    "lin","lin_interactions","lin_interactions_subset","poly","spline","lasso",
    "ridge"
  ),
  RMSE = c(2.077942, 2.013816, 2.166138, 2.072878,2.115911,2.068582, 2.080013)
)

kable(model_summary)
```

```{r, echo = FALSE}
#COULD DELETE THIS MODEL
lm.fit <- lm(log(sd) ~ (log_St+log_Re+logistic_Fr)^2, data)
# plot(lm.fit)
# summary(lm.fit)
summary(lm.fit)
```

```{r, echo= FALSE}
sd_model <- lm(log(sd) ~ log_St + log_Re + logistic_Fr +
              log_St:log_Re + log_St:logistic_Fr + log_Re:logistic_Fr,
              data = data)
summary(sd_model)
```

```{r, echo = FALSE}
newdata_sd <- data.frame(
  log_St = seq(min(data$log_St), max(data$log_St), length.out = 100),
  log_Re = mean(data$log_Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdSt <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdSt <- cbind(newdata_sd, pred_sdSt)

plot_sdSt1 <- ggplot(plot_sdSt, aes(x= log_St, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (log(SD))",
    x = "log(St)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  log_Re = seq(min(data$log_Re), max(data$log_Re), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred_sdRe <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdRe <- cbind(newdata_sd, pred_sdRe)

plot_sdRe1 <- ggplot(plot_sdRe, aes(x= log_Re, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "log(Re)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()

newdata_sd <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  log_St = mean(data$log_St, na.rm = TRUE),
  log_Re = mean(data$log_Re, na.rm = TRUE)
)

pred_sdFr <- predict(sd_model, newdata = newdata_sd, interval = "confidence", level = 0.95)
plot_sdFr <- cbind(newdata_sd, pred_sdFr)

plot_sdFr1 <- ggplot(plot_sdFr, aes(x= logistic_Fr, y = fit)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic(Fr)",
    y = "Predicted log(sd)"
  ) +
  theme_minimal()
```

```{r, fig.width=9, fig.height=3}
(plot_sdSt1 | plot_sdRe1 | plot_sdFr1)
```

The plots depict the 95% CI for the predicted log(sd) as each predictor varies, given that the other two predictors are held at their average values. For Re, we are fairly confident that there appears to be a positive relationship between log(St) and log(SD), with greater variability near the extremes. For Re, we are very confident that there is a strong negative relationship between log(Re) and log(SD), such that at larger Re values, there is less variability in log(Re). For Fr, we have some confidence that the relationship between logistic(Fr) and log(SD) is negative; however, the confidence band is much wider compared to the other predictors, which means that there does appear to be more uncertainty.
```{r}
summary(lm.fit)
```

From our model, we can see that log_Re, logistic(Fr), and the interaction between log(R) and logistic(Fr) has significant effects on the standard deviation of the particle cluster volume distribution. For example, holding other factors constant, for a 1% increase in the Reynold's Number, there would be a 4.915% in decrease standard deviation. Also, holding other factors constant, for a 1% increase in the Stokes number, there would be a 20.704% decrease in standard deviation. In context, this means that more turbulent regimes and regimes with more dense particles lead to narrower particle cluster distributions. However, the effect with Reynold's Number is reduces in regimes with higher gravitational acceleration, or Fr, as seen with the interaction effect between log(Re) and logistic(Fr).


# Modeling Skew

```{r}
# Boxcox transformation for skew
boxcox(lm(data$skewness ~ 1))


```

Using the Box-Cox transformation above, it is clear that the $\lambda$ which maximizes the log-likelihood is around 0.5 which indicates a square root transformation on the response is optimal.

## Interpretable Model

```{r}
# Building the more interpetable model
skew_m1 <- lm(sqrt(skewness) ~ sqrt(St) + as.factor(Re) + as.factor(logistic_Fr) + 
                sqrt(St):as.factor(Re) + sqrt(St):as.factor(logistic_Fr) + 
                as.factor(Re):as.factor(logistic_Fr), 
         data = data)

# Getting the summary for the model 
skew_int <- summary(skew_m1)
skew_int
```

Above, a linear model is fit with the predictors which were determind to have the best model output in ridge regression.

```{r}
# Model diagnostics for interpretable model
cat("R²:", skew_int$r.squared, "\n")
cat("Adjusted R²:", skew_int$adj.r.squared, "\n")
cat("AIC:", AIC(skew_m1), "\n")
cat("BIC:", BIC(skew_m1), "\n")

```

```{r}
# Determining the RMSE
y <- sqrt(data$skewness)
y_hat <- fitted(skew_int)
resid <- residuals(skew_int)

RMSE <- sqrt(mean(resid^2))
MSE  <- mean(resid^2)
bias <- mean(resid)

c(RMSE = RMSE, MSE = MSE, Bias = bias)

```

```{r}
# Finding the Cp
n <- length(m1$fitted.values)
p <- length(coef(skew_m1))       # includes intercept
SSE <- sum(resid(skew_m1)^2)
sigma2 <- summary(skew_m1)$sigma^2

Cp <- SSE / sigma2 - n + 2 * p
Cp

cat("Mallows' Cp:", Cp, "\n")

# Printing the anova test
anova(m1)
```

```{r}
par(mfrow = c(2, 2))
plot(skew_m1)
```

The residual plot appears as though the residuals are fairly evenly distributed above and below 0.

Next, a confidence band is fit around the model parameters when Re and Fr are not treated as categorical variables.

```{r}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
skew_int2

```

```{r}
# Using a model without the as.factor
skew_m2 <- lm(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)
skew_int2 <- summary(skew_m2)
#skew_int2

# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p1 <- ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p2 <- ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

p3 <- ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Combine the plots
(p1 | p2 | p3)
```

Given the nature of the three levels for both Re and Fr, there is a large degree of uncertainty for these predictors when predicting the response which is especially evident in the wide nature of the 95% confidence intervals near the extremes of both of these predictors. The below plots show slices of each of the predictors for the response at the mean of the other two predictors which aren't on the x-axis. Receiving additional data in the future would address the high amount of uncertainty in the model as we continue to refine the model.

```{r}
# Confidence interval dataframe
skew_ci <- cbind(
  data,
  predict(skew_m2, interval = "confidence", level = 0.95)
)

# Create a new data frame varying Re while fixing others at their mean
newdata <- data.frame(
  Re = seq(min(data$Re), max(data$Re), length.out = 100),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

# Predict fit + 95% CI for these Re values
pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

ggplot(plot_df, aes(x = Re, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "Re",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Doing the same for St
newdata <- data.frame(
  St_sqrt = seq(min(sqrt(data$St)), max(sqrt(data$St)), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  logistic_Fr = mean(data$logistic_Fr, na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

ggplot(plot_df, aes(x = St_sqrt, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "sqrt(St)",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()

# Same for logistic_Fr
newdata <- data.frame(
  logistic_Fr = seq(min(data$logistic_Fr), max(data$logistic_Fr), length.out = 100),
  Re = mean(data$Re, na.rm = TRUE),
  St_sqrt = mean(sqrt(data$St), na.rm = TRUE)
)

pred <- predict(skew_m2, newdata = newdata, interval = "confidence", level = 0.95)
plot_df <- cbind(newdata, pred)

ggplot(plot_df, aes(x = logistic_Fr, y = fit)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15, fill = "gray") +
  labs(
    title = "95% Confidence Band for Fitted Mean (sqrt(skewness))",
    x = "logistic_Fr",
    y = "Predicted sqrt(skewness)"
  ) +
  theme_minimal()
```

```{r}
# Assessing model fit for linear model that isn't categorical
cat("R²:", skew_int2$r.squared, "\n")
cat("Adjusted R²:", skew_int2$adj.r.squared, "\n")
cat("AIC:", AIC(skew_m2), "\n")
cat("BIC:", BIC(skew_m2), "\n")
```

The model is then plotted with the associated confidence intervals to assess certainty for values that are being interpolated.

The red lines above show the true values for both Re and logistic_Fr so the rest of the values are interpolated. The confidence interval is tighter in the middle of the Re and logistic_Fr distributions, but gets wide as we grow more uncertain around the tails of the distribution.

After fitting several model types, ridge regression performed the best. Below, is the output from the ridge regression.

```{r}
# Basic ridge model
set.seed(325)
y <- data$skewness
X <- model.matrix(sqrt(skewness) ~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr, 
         data = data)[, -1]  

# Fitting the ridge model
ridge_fit <- cv.glmnet(X, y, alpha = 0, nfolds = 5)

```

The fit of the ridge model was then assessed using a residual plot.

```{r}
# Building the residual plot
# Predicted values
y_pred <- predict(ridge_fit, newx = X, s = "lambda.min")
residuals <- y - y_pred

# Residual plot
plot(y_pred, residuals, 
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residual Plot for Ridge Regression")
abline(h = 0, col = "black", lwd = 2)

# Adding a nonlinear trend to the residuals
ord <- order(y_pred)
lo <- loess(residuals ~ y_pred)
lines(y_pred[ord], predict(lo)[ord], col = "blue", lwd = 2, lty = 2)
```

To check assess the extent to which the model is overfit, cross-validation was used to evaluate the model's performance on a hold out set where k = 5.

```{r}
# Checking for model overfitting
set.seed(325)

y_ridge <- sqrt(data$skewness)
X_ridge <- model.matrix(~ St_sqrt + Re + logistic_Fr + St_sqrt:Re + 
               St_sqrt:logistic_Fr + Re:logistic_Fr,
                        data = data)[, -1]  
n <- nrow(X)

# Create folds (rough random splitting)
fold_ids <- sample(rep(1:k, length.out = n))

# Initializing variables
y_true_all <- numeric(0)
y_pred_all <- numeric(0)
fold_errors <- numeric(0)

# Loop folds
for (fold in 1:k) {
  test_idx <- which(fold_ids == fold)
  train_idx <- setdiff(seq_len(n), test_idx)

  # Splitting into train and test data sets
  X_train <- X_ridge[train_idx, , drop = FALSE]
  y_train <- y_ridge[train_idx]

  X_test <- X_ridge[test_idx, , drop = FALSE]
  y_test <- y_ridge[test_idx]

  # Fit the new models
  cv_fit <- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 5)   
  lambda_star <- cv_fit$lambda.min

  # Making prediction on test set
  preds <- as.numeric(predict(cv_fit, newx = X_test, s = "lambda.min"))

  # Collecting predictions results
  y_true_all <- c(y_true_all, y_test)
  y_pred_all <- c(y_pred_all, preds)
  
  # Individiual cv errors
  fold_errors[fold] <- mean((y_test - preds)^2)
}

```

```{r}
## Model diagnostics

# R^2
SSE <- sum((y_true_all - y_pred_all)^2)
SST <- sum((y_true_all - mean(y_true_all))^2)
R2_overall <- 1 - SSE / SST

# RMSE
resid_cv <- y_true_all - y_pred_all
RMSE <- sqrt(mean((resid_cv)^2))

# Average CV MSE
mse_cv <- mean(fold_errors)
cv_sd <- sd(fold_errors)

metrics <- list(R2 = R2_overall, RMSE = RMSE, 
                SD = sd(resid))
metrics
cat("5-fold CV MSE:", mean(fold_errors), "±", sd(fold_errors), "\n")

```

This model before cross validation has an $R^2$ of 0.477 which is fairly close to the $R^2$ = 0.537 of the model above. The RMSE is also fairly low across the models which means that on average the predictions aren't too far from the true values of the variables.
